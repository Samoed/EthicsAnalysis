#+STARTUP: latexpreview
#+TITLE: Разработка системы для автоматического сбора, анализа и визуализации информации по этичности компаний
#+AUTHOR: Соломатин Роман Игоревич
#+LANGUAGE: ru
#+cite_export: biblatex
#+OPTIONS: toc:nil H:4 ':t
#+BEGIN_COMMENT
#LATEX_CLASS: HSEUniversity
#LATEX_HEADER_EXTRA: \supervisor{к.т.н.}{доцент кафедры информационных технологий в бизнесе НИУ ВШЭ-Пермь}{А. В. Бузмаков}
#+END_COMMENT
#+LATEX_CLASS: HSEUniversityPractice
#+LATEX_CLASS_OPTIONS: [PI, VKR]
#+LATEX_HEADER_EXTRA: \supervisor{к.т.н.}{доцент кафедры ИТБ}{A.~В.~Кычкин}
#+LATEX_HEADER_EXTRA: \Abstract{В данной работе проведен анализ этичности разных компаний на основе отзывов клиентов, которые были собранны с сайтов bani.ru, sravni.ru, vk.com. Данный анализ основан на алгоритмах обработки естественного языка, в частности с помощью модели BERT.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В первой главе находится описание используемых алгоримов.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: Во второй главе представлено проектирование системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В третьей главе представлена реализация системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В четвертой главе представлено тестирование работы системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: Количество страниц -- \pageref*{pg:end}, количество иллюстраций -- \TotalValue{totalfigures}, количетсво таблиц -- \TotalValue{totaltables}.
#+LATEX_HEADER_EXTRA: }
#+COMMENT: Header extra иначе не генерит нормально previews
#+COMMENT: Написать аннотацию. Как-то сделать ограничения (упор) на отзывы клиентов

* Введение
:PROPERTIES:
:UNNUMBERED: t
:END:
Эффективность работы компаний зависит от многих факторов, в том числе и от этики делового поведения и ответственности их сотрудников. Компании, которые придерживаются высоких стандартов этики и интегрируют их в свою культуру, обычно имеют более лояльных клиентов и успешнее конкурируют на рынке. Кроме того, соблюдение этических норм и принципов способствует укреплению репутации компании, что может привести к привлечению талантливых сотрудников и установлению долгосрочных партнерских отношений с другими компаниями и организациями. В целом, этика делового поведения играет важную роль в формировании имиджа компании и ее успеха на рынке.

Этика компаний – это разделяемые всеми сотрудниками организации правила и нормы, ценности и убеждения, манера общения и другие факторы, которые регламентируют поведение и взаимодействии членов компании. Существует 3 уровня этики компаний\nbsp{}[cite:@smirnova_biznesetika_2021]:
1. Мировой -- отвечает за увеличение общественного благосостояния, обеспечение рабочих мест, научно-технические инновации и модернизацию производственных процессов и так далее.
2. Макроуровень -- отвечает за принципы рыночной конкуренции, информационной прозрачность и равнодоступности для всех участников рынка и так далее.
3. Микроуровeнь -- отвечает за доверие и отсутствие дискриминации в отношениях между контрагентами, между сотрудниками и менеджерами, морально-нравственный климат в организации и так далее.

В данной работе будет рассматриваться этика на микроуровне, как вероятность принятия компанией каких-то действий, которые краткосрочно не обязательно выигрышных для бизнеса, но которые увеличивают лояльность клиентов компании. Например, у клиента банка задержали зарплату и он не делает платеж по кредиту. Формально банк может по кредитному договору назначить штраф за неисполнение клиентом обязательств, но войдя в положение клиента, банк может не назначить или отменить такой штраф.

В настоящее время существует несколько сервисов, которые призваны оценивать этику компании на основании финансовых показателей[fn:1] и судебных дел[fn:2]. Это привело к ситуации, когда отдельные лица должны проводить свои собственные исследования, чтобы определить насколько этична компания. Это часто включает в себя просмотр отзывов с различных веб-сайтов, что может занять много времени и не всегда может дать исчерпывающую или точную картину, так как не включает в себя качество обслуживания.

Для решения этой проблемы реализована система, которая собирает и анализирует отзывы потребителей с различных веб-сайтов, чтобы дать более полную и точную оценку этической практики компании. Затем собранные данные анализируются с помощью различных методов, таких как обработка естественного языка и машинного обучения, для выявления закономерностей и тенденций, связанных с этической практикой компании. Полученный анализ может быть использован для разработки более надежной и достоверной системы оценки этичности компаний.

Объект исследования -- методы оценки этичности компаний.

Предмет исследования -- автоматизация оценки этичности компаний на основании отзывов клиентов.

Цель работы – создание системы для оценки этичности компаний.

Исходя из поставленной цели, необходимо:

1. Провести анализ предметной области и требований.
3. Реализовать систему.
4. Провести тестирование системы.

Этап анализа должен:
1. Анализ предметной области.
2. Анализ требований к системе.
3. Анализ существующих алгоритмов.

Этап проектирования должен включать:
1. Проектирование серверной части.
2. Проектирование модели для определения этичности.
3. Проектирование клиентской части приложения.

Этап реализации должен включать:
1. Описание сбора данных.
2. Реализации модели.
3. Реализации серверной части.
4. Реализации клиентской части.

Этап тестирования должен включать:
1. Тестирование модели.
2. Тестирование серверной части.
3. Тестирование клиентской части.

В ходе выполнения анализа, проектирования и реализации приложения используется объектно-ориентированный подход. Результаты анализа и решения задач проектирования формализуются с помощью диаграмм =UML=. При разработке базы данных используется реляционная СУБД =PostgreSQL=, а серверная часть приложения реализуется на языке python\nbsp{}[cite:@vanrossum_python_2009] с помощью фреймворка =FastApi=, а алгоритмы анализы текста будут использовать методы машинного обучения.
* Анализ предметной области
В данной главе представлен аналитический обзор оценок этичности компаний и алгоритмов машинного обучения, а также обзор существующих программных решений для поставленной проблемы.

Анализ предметной области следует разделить на следующие пункты:
1. Анализ процесса определения этичности компаний сейчас позволяет понять, как этот процесс сейчас происходит и как его лучше всего автоматизировать.
2. Анализ оценок этичности компаний для того, чтобы в дальнейшем определить этичность компаний.
3. Анализ существующих решений выполняется с целью выделения их сильных и слабых сторон по отношению к решаемой проблеме и обоснования необходимости разработки нового средства, подходящего под регламент задач.
4. Анализ алгоритмов позволяет понять с помощью каких алгоритмов можно найти полезную информацию в текстах.
5. Анализ требований к системе позволит выделить функциональные и нефункциональные требования.
** Анализ определения этичности компании
Этичность компаний уже давно вызывает озабоченность, особенно их поведение в спорных ситуациях и предоставление услуг, ориентированных на клиента. В последние годы все большее внимание уделяется оценке этичности компаний\nbsp{}[cite:@mure_esg_2021; @semenko_korporativnaya_2022; @kudryavceva_korporativnosocialnaya_2016], особенно в банковском секторе и через призму экологических, социальных и управленческих факторов (ESG). Необходимость в таких оценках становится все более острой по мере того, как общество продолжает бороться с последствиями неправомерных действий корпораций и более широким воздействием корпоративной деятельности на общество и окружающую среду.

Сейчас процесс поиска этичной компании выгладит следующим образом: сначала ищутся компании, которые предоставляют желаемые услуги. Далее они изучаются, чтобы определить их этичность. Этот процесс включает в себя:
1. Просмотр отчетности компании.
2. Анализ ее финансовой деятельности.
3. Изучение информации о социальной ответственности.

Для этого клиенты компаний обращаются к различным источникам информации, таким как веб-сайты компаний, рейтинговые агентства, исследовательские организации и другие источники. Потом, изучаются социальные сети компании или отзывы пользователей на разных сайтах, форумах и социальных сетях, чтобы получить дополнительную информацию и оценить общее мнение о компании. После изучения каждой компании люди выбирают ту, которую они считают наиболее этичной и социально ответственной. Блок-схема данного поиска рис.\nbsp{}[[ref:fig:as_is]]. Важным фактором для определения этичности компании может быть ее социальная ответственность, устойчивость бизнеса и соблюдение норм и стандартов в области финансовой деятельности.

В целом, процесс поиска компаний и определения их этичности может быть длительным и требует серьезного подхода. Люди могут использовать различные источники информации, чтобы сделать осознанный выбор и инвестировать свои деньги в компанию, которая соответствует их ожиданиям и требованиям.
#+begin_src mermaid :file img/mermaid/as_is.png :results output :theme neutral
flowchart TD
    direction TB
    A[Поиск компаний, которые предоставляют желаемые услуги]
    A --> B[Составление списка компаний, которые предоставляют услуги]
    B --> C
    subgraph search[ ]
        C{{Изучение каждой компании}}
        C --> D[Просмотр отчетности]
        D --> E[Изучение соцсетей компании]
        E --> F[Просмотр отзывов на разный сайтах]
        F --> C
    end
    F --> G[Выбор компании]
#+end_src

#+NAME: fig:as_is
#+CAPTION: Диаграмма того, как сейчас происходит поиск компании
#+ATTR_LATEX: :width 0.6\textwidth :placement [h]
#+RESULTS:
[[file:img/mermaid/as_is.png]]

** Анализ оценок этичности компаний
Оценка этики компании -- это не одноразовый процесс, а скорее непрерывная попытка понять и оценить действия, политику и практику компании с течением времени. Это включает в себя рассмотрение соблюдения компанией отраслевых этических стандартов и передовой практики, а также мониторинг любых изменений в этической позиции компании с течением времени. Кроме того, участие в диалоге с компанией и консультации с организациями, специализирующимися на оценке корпоративной ответственности могут дать ценную информацию об этических практиках компании.

Компаниям важно оставаться этичными, так как на долгосрочной перспективе это приносит большую прибыль и улучшает показатели бизнеса, чем неэтичный способ ведение бизнеса[cite:@climent_ethical_2018; @mure_esg_2021]. Насколько этична компания можно рассматривать с двух сторон, самой компании и их клиентов. Со стороны компаний можно выделить факторы, которые можно получить из их отчетности:
- количество капитала, чтобы они не могли обанкротиться.
- какое влияние они вносят на окружающую среду.
- куда идут инвестиции\nbsp{}[cite:@harvey_ethical_1995].
#+COMMENT: метрики качества сервисов, как сравнивать
Для пользователей одними из ключевых факторов можно выделить:
- качество пользовательского сервиса\nbsp{}[cite:@brunk_exploring_2010], как правило пользователи оставляют отзывы на сайтах по пяти бальной шкале.
- насколько навязчивые услуги компании\nbsp{}[cite:@mitchell_bank_1992], как правило пользователи оставляют отзывы на сайтах по пяти бальной шкале.

#+COMMENT: Этическая репутация банка представляет собой концепцию, которая отражает общее мнение о том, насколько данный банк соблюдает этические нормы в своей деятельности. В отличие от общего сентимента, который может быть связан с различными аспектами банковского бизнеса, такими как дизайн карты или офиса, этическая репутация фокусируется исключительно на этическом поведении банка. Это включает соблюдение этических норм, таких как отсутствие обмана, неприемлемого использования власти, вежливости и прочих схожих аспектов. В многих случаях, общее мнение о банке является сущностным вопросом этического сентимента, поскольку потребитель может ценить добросовестное общение, отсутствие обмана, адекватность цен, а также высокое качество предоставляемых услуг, что, в свою очередь, свидетельствует о соблюдении этических норм бизнеса.

#+COMMENT: ????? отчетность компаний
В данной работе этичность компаний будет определяться по отзывам клиентов, которые освещают проблемы качества услуг и качество сервиса, и на основе отчетности компаний, что позволит полностью осветить проблему. Для анализа текстов будут использоваться алгоритмы машинного обучения.
** Анализ существующих решений
Существует несколько индексов, предназначенных для измерения этичности -- индекс Доу Джонса (DJSI)\nbsp{}[cite:@lopez_sustainable_2007] и FTSE4GOOD\nbsp{}[cite:@collison_financial_2008].

DJSI оценивает показатели устойчивости компаний различных секторов на основе экономических, экологических и социальных критериев. Компании отбираются на основе их показателей по сравнению с аналогичными компаниями в том же секторе. Процесс оценки включает в себя тщательную оценку компаний по различным критериям, включая корпоративное управление, экологический менеджмент, трудовую практику, права человека и социальные вопросы.

Аналогичным образом, индекс FTSE4GOOD предназначен для оценки деятельности компаний, которые демонстрируют эффективную практику экологического, социального и управленческого менеджмента (ESG). Компании отбираются на основе их практики ESG и оцениваются по различным критериям, включая изменение климата, права человека и корпоративное управление.

Индексы DJSI и FTSE4GOOD разработаны для того, чтобы помочь инвесторам определить компании, которые привержены этической практике. Эти индексы предоставляют инвесторам стандартизированный способ сравнения компаний на основе их показателей. Это помогает инвесторам принимать более обоснованные инвестиционные решения и побуждает компании внедрять устойчивую практику для привлечения инвестиций.

Для российских компаний нет аналогичных индексов. Сейчас данные об этичности компаний можно получить из агрегаторов отзывов и отчётности. Агрегаторы позволяют собрать информацию о клиентском обслуживании, а отчетность компаний о положении дел в целом. Но сейчас не существует способов, как можно оценить все вместе.
#+COMMENT: Расисать, чем не устраивают
** Алгоритмы для анализа текста
#+COMMENT: 42 мин. Откуда вооб  ще появляются алгоритмы
Алгоритмы машинного обучения для анализа текста получили широкое распространение для извлечения информации из неструктурированных данных с помощью больших помеченных наборов данных. Среди различных используемых методов несколько алгоритмов оказались особенно эффективными в этой области. К ним относятся мешок слов\nbsp{}[cite:@harris_distributional_1954], TF-IDF\nbsp{}[cite:@jones_karen_sparck_statistical_1972], Word2Vec\nbsp{}[cite:@mikolov_distributed_2013], ELMO\nbsp{}[cite:@peters_deep_2018], GPT\nbsp{}[cite:@radford_language_2019], BERT\nbsp{}[cite:@devlin_bert_2019] и другие. Каждый из этих алгоритмов обладает уникальными характеристиками, которые делают их хорошо подходящими для определенных задач.

Модель "Мешок слов" представляет текстовые данные путем присвоения уникального номера каждому слову в документе. Этот метод прост в реализации, но не учитывает порядок слов в предложении. С другой стороны, модель TF-IDF представляет текстовые данные, учитывая как частоту слова в документе (TF), так и его редкость во всех документах корпуса (IDF). Этот подход может быть использован для определения важности слова в данном документе и обычно используется в задачах поиска информации и обработки естественного языка, но он не использует контекста употребления слов, не имея возможность различать в том числе омонимию.

Word2Vec использует векторное представление слов, что позволяет алгоритму улавливать значение слов в сходных контекстах. Это позволяет более точно представлять взаимосвязи между словами, что приводит к повышению производительности в таких задачах, как классификация текста и анализ настроений. Однако, этот метод все ещё неспособен различать смысл слова в зависимости от конеткста, так как каждое слово имеет единственную "кодировку".

ELMO, GPT и BERT, с другой стороны, основаны на архитектуре трансформеров, в которой каждое предложение представлено вектором чисел, обычно известным как вложение. Такое представление позволяет получить более полное и целостное понимание текста, поскольку оно учитывает контекст всего предложения или текста.

Из этих алгоритмов BERT считается наиболее продвинутым и мощным, поскольку он способен учитывать контекст всего предложения или текста, в то время как GPT и ELMO рассматривают только односторонний контекст. Это позволяет BERT достигать самых современных результатов в широком спектре задач анализа естественного языка.

Таблица результата сравнения моделей [[tbl:model_compare]].
#+COMMENT: добавить про скорость работы с текстом

#+NAME: tbl:model_compare
#+CAPTION: Сравнение моделей
#+ATTR_LATEX: :align |c|c|c| :placement [h!]
|------------+-------------------------------+------------------|
| Модель     | Вектор слов                   | Контекст         |
|------------+-------------------------------+------------------|
| Мешок слов | зависит от количества слов    | нет              |
|------------+-------------------------------+------------------|
| TF-IDF     | зависит от количества слов    | очень слабо      |
|------------+-------------------------------+------------------|
| Word2Vec   | не зависит от количества слов | слабо            |
|------------+-------------------------------+------------------|
| ELMO       | не зависит от количества слов | однонаправленный |
|------------+-------------------------------+------------------|
| GPT        | не зависит от количества слов | однонаправленный |
|------------+-------------------------------+------------------|
| BERT       | не зависит от количества слов | двунаправленный  |
|------------+-------------------------------+------------------|

*** BERT
BERT\nbsp{}[cite:@devlin_bert_2019] (Bidirectional Encoder Representations from Transformers) -- это нейросетевая языковая модель, которая относится к классу трансформеров. Она состоит из 12 «базовых блоков» (слоев), а на каждом слое 768 параметров.

На вход модели подается предложение или пара предложений. Затем разделяется на отдельные слова (токены). Потом в начало последовательности токенов вставляется специальный токен =[CLS]=, обозначающий начало предложения или начало последовательности предложений. Пары предложений группируются в одну последовательность и разделяются с помощью специального токена =[SEP]=, затем к каждому токену добавляется эмбеддинг, показывающий к какому предложению относится токен. Потом все токены превращаются в эмбеддинги\nbsp{}[[fig:inputemebeddings]] по механизму описанному в работе\nbsp{}[cite:@vaswani_attention_2017].

#+CAPTION: Пример ввода текста в модель
#+NAME: fig:inputemebeddings
#+ATTR_LATEX: :placement [h]
[[file:img/Input_Emebeddings.pdf]]

При обучении модель выполняет на 2 задания:
 1) Предсказание слова в предложении

    Поскольку стандартные языковые модели либо смотрят текст слева направо или справа налево\nbsp{}[[fig:BERT_comparisons]], как ELMo\nbsp{}[cite:@peters_deep_2018] и GPT\nbsp{}[cite:@radford_language_2019], они не подходят под некоторые типы заданий. Так как BERT двунаправленный, у каждого слова можно посмотреть его контекст, что позволит предсказать замаскированное слово.

    #+CAPTION: Сравнение принципов работы BERT, ELMo, GPT
    #+NAME: fig:BERT_comparisons
    #+ATTR_LATEX: :placement [h]
    [[file:img/BERT_comparisons.pdf]]

    Это задание обучается следующим образом -- 15% случайных слов заменяются в каждом предложении на специальный токен =[MASK]=, а затем предсказываются на основании контекста. Однако иногда слова заменяются не на специальны токена, в 10% заменяются на случайный токен и еще в 10% заменяются на случайное слово.

 2) Предсказание следующего предложения

    Для того чтобы обучить модель, которая понимает отношения предложений, она предсказывает, идут ли предложения друг за другом. Для этого с 50% вероятностью выбирают предложения, которые находятся рядом и наоборот. Пример ввода пары предложений в модель\nbsp{}[[fig:bert_pretrainin]].

    #+CAPTION: Схемам работы BERT
    #+NAME: fig:bert_pretrainin
    #+ATTR_LATEX: :width 0.6\textwidth :placement [hbp]
    [[file:img/bert_pretrainin.png]]
*** Sentence BERT
Sentense BERT\nbsp{}[cite:@reimers_sentence-bert_2019] -- это модификация предобученных моделей BERT, которая обрабатывает два предложения, затем усреднят их, а после с помощью функции ошибки выдаёт результат. Схема работы модели\nbsp{}[[ref:fig:sbert]].
#+CAPTION: Схема работы SBERT
#+NAME: fig:sbert
#+ATTR_LATEX: :width 0.6\textwidth :placement [h!]
[[file:img/sbert.png]]
Основное преимущество данной модели над классическим BERT: эмбеддинги предложений можно сравнивать друг с другом независимо и не пересчитывать их пару каждый раз. Например, если для поиска похожих предложений из 10000 для обычного BERT потребуется 50 миллионов вычислений различных пар предложений, и это займёт несколько дней часов, то Sentense BERT рассчитает эмбеддинг каждого предложения отдельно, потом их сравнит. Такой способ рассчета ускоряет работу программы до нескольких секунд.
** Анализ требований к системе
Исходя из интервью с заказчиком система должна уметь:
1. Показывать историю изменений индекса с возможностью фильтровать по:
   1. Годам.
   2. Отраслям компаний, с возможностью множественного выбора.
   3. Компаниям, с возможностью множественного выбора.
   4. Моделям, с возможностью множественного выбора.
   5. Источникам, с возможностью множественного выбора.
2. Агрегировать значения индекса по годам и кварталам.
3. Анализировать тексты для построения индекса этичности.
4. Иметь возможность добавления анализа текста несколькими вариантами.
5. Сохранять тексты для последующего анализа другими методами.
6. Система должна собирать данные с сайтов banki.ru, sravni.ru и комментарии из групп "вконтаке".
8. На сайте должен быть график, который показывать изменение индекса этичности компаний и количества собранных отзывов по разным источникам.
7. Для расчета индекса этичности компаний на основании рецензий должна использоваться формула\nbsp{}[[ref:eq:ethics]]:

#+NAME: eq:ethics
\begin{equation}
\begin{aligned}
\text{Base index} &= \frac{\text{positive} - \text{negative}}{\text{positive} + \text{negative}} \\
\text{Std index} &= \sqrt{\frac{\text{positive}}{\text{negative} \cdot (\text{positive} + \text{negative})^{3}} + \frac{\text{negative}}{\text{positive} \cdot (\text{positive} + \text{negative})^{3}}} \\
\text{Index} &= ({2\cdot({\text{Base index}}-{\text{Mean index}} > 0) - 1})\cdot\\
            &{max\left(\left|{\text{Base index}}-{\text{Mean index}}\right|-{\text{Std index}}, 0\right)}
\end{aligned}
\end{equation}

$positive$ -- количество позитивных предложений,

$negative$ -- количество негативных предложений,

$Mean\ index$ -- среднее значения для пар источник сбора данных и модели, которая обрабатывала предложения.

На основе описания функциональных требований была создана диаграмма вариантов использования, которая представлена на рисунке\nbsp{}[[ref:fig:usecasefull]].
#+NAME: fig:usecasefull
#+CAPTION: Диаграмма вариантов использования
#+ATTR_LATEX: :placement [h!] :width \textwidth
[[file:img/use-case.png]]

Также были получены нефункциональные требования:
1. Построение графика не должно занимать больше секунды.
2. Данные должны собираться автоматическ.
3. Данные должны обрабатываться автоматически.
4. Система должны способна работать с большим объемом информации.
5. Система должна быть стабильна.
** Выбор технологий для разработки
Для реализации этой системы будет использоваться язык Python. Для этого языка разработано много библиотек, которые позволят быстро реализовать алгоритмы обработки естественного языка, в частности в этом проекте будет использоваться Pytorch\nbsp{}[cite:@paszke_pytorch_2019] и HuggingFace\nbsp{}[cite:@wolf_transformers_2020], и собирать данные с сайтов. Для реализации API будет использоваться FastAPI, что позволит разработать API для системы с автоматической генерацией документации.

Хранение данных будет использоваться объектно-реляционная система управления базами данных PostgreSQL, что позволит обрабатывать большие объемы данных. Для работы с ней будет использоваться Code first подход, с помощью Python библиотек Sqlalchemy и Alembic для изменения схемы данных (миграций).
** Выводы по главе
По итогам анализа предметной области, можно сделать вывод о том, что определение этичности компаний является важной задачей, которую можно автоматизировать с помощью алгоритмов машинного обучения. Анализ оценок этичности компаний позволяет понять, какие факторы необходимо учитывать при разработке алгоритмов. Обзор существующих решений показал, что некоторые из них имеют свои преимущества и недостатки, и может потребоваться разработка нового средства, учитывающего особенности задачи. Анализ алгоритмов помогает выбрать наиболее подходящие алгоритмы для поиска полезной информации в текстах. Наконец, анализ требований к системе позволяет определить необходимые функциональные и нефункциональные требования, которые будут учитываться при разработке решения. В целом, эти аналитические пункты помогут определить оптимальный подход к решению задачи определения этичности компаний.
* Проектирование системы
В данной главе представлена общая архитектура системы и каждого модуля. Описана база данных и архитектура каждого модуля отдельно.

Этап проектирования следует разделить на следующие пункты:
1. Определение основных компонентов приложения и проектирование архитектуры системы.
2. Проектирование базы данных и модулей.
3. Проектирование модели для обработки естественного текста.

Данная глава предоставляет описание системы, продемонстрировать каждый компонент и их взаимосвязь в достижении желаемого результата.
** Проектирование архитектуры системы
Система будет разделена на отдельные независимые компоненты (микросервисы), что позволит ей быть надежной (если в какой-то части системы будут сбои, то остальная часть системы продолжит работать) и масштабируемой (легко добавлять новые компоненты). Каждый микросервис системы будет представлять собой docker container, которые будут управляться с помощью docker compose. Каждый сервис будет реализовывать отдельный компонент бизнес-логики и коммуницировать с другими компонентами через REST API.

Было выделено 4 главных компонента бизнес логики:
1. Работа с базой данных -- это HTTP API, который обеспечивает возможность сохранения и получения данных из базы данных. Данный компонент принимает запросы на сохранение данных, получение информации из базы данных и возвращает результаты обработки этих запросов.
2. Сбор данных -- компонент, который отвечает за сбор информации с нескольких источников. Для этого используется несколько независимых сборщиков данных, которые работают с различными сайтами и другими источниками.
3. Обработка данных -- данный компонент содержит несколько моделей, которые используются для анализа данных. Эти модели производят различные виды анализа, от простой фильтрации и сортировки до более сложных операций анализа и прогнозирования.
4. Агрегирование данных -- этот компонент отвечает за агрегацию обработанных данных в единый индекс. Данный индекс может быть использован для удобного представления полученных результатов в виде отчетов и графиков. Данный модуль нужен для того чтобы быстро получать новые графики, так как агрегирование всех отзывов для компаний может занимать много времени.

Результат архитектуры системы на рис.\nbsp{}[[ref:fig:architecture]].

#+NAME: fig:architecture
#+CAPTION: Диаграмма архитектуры системы
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/architecture.png]]

Сервис для работы с базой данных, который будет обеспечивать сохранение и получение информации из различных сервисов сбора и обработки данных. Для этого будет предоставлен API, который будет использоваться для отправки и получения данных.

Сервисы сбора данных будут отправлять собранные тексты в формате JSON на сервис работы с базой данных с помощью HTTP запросов. Кроме того, информация, необходимая для сбора данных, будет храниться в базах данных соответствующих сервисов.

Сервис агрегации данных будет периодически обновлять базу данных один раз в день для обеспечения актуальности данных.

Сервис сбора данных будет включать несколько моделей машинного обучения, которые будут использоваться для анализа данных, полученных из сервиса сбора данных. После обработки данных, результаты будут отправляться обратно в сервис сбора данных.
** Проектирование базы данных
Исходя из поставленных требований было решено разделить базу данных на 2 подчасти:
1. Основная база данных будет хранить данные.
2. База данных для агрегации будет позволять быстро получать агрегированные данные.

*** Проектирование основной базы данных
На основании требований была разработана следующая схема базы данных:

Таблица сфер компаний позволяет в дальнейшей удобно фильтровать данные в зависимости от типа компании.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица сфера компании\label{tbl:company_type}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*       | *Тип*    | *Описание*                 |
| Идентификатор  | Целое  | Уникальный идентификатор |
| Сфера компании | Строка |                          |

#+ATTR_LATEX: :environment longtblr :options caption={Таблица компаний\label{tbl:companies}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*           | *Тип*    | *Описание*                                                                 |
| Идентификатор      | Целое  | Уникальный идентификатор                                                 |
| Название компании  | Строка |                                                                          |
| Описание компании  | Строка | Дополнительное поле для сохранения вспомогательной информации о компании |
| Лицензия компании  | Строка | По лицензии компаний может будет сопоставлять компании на разных сайтах  |
| Код сферы компании | Целое  | Внешний ключ из таблицы Сфера компании                                   |

Аналогично для сфер компаний таблица для типов источников позволяет удобно работать с данными в дальнейшем.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица тип источников\label{tbl:source_type}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                | *Тип*    | *Описание*                 |
| Идентификатор           | Целое  | Уникальный идентификатор |
| Название типа источника | Строка |                          |

Таблица источников будет хранить информацию об источниках и когда было последнее обновление данных для них (в полях "состояние сборщика данных" и "дата последнего сбора данных"). Поле "состояние сборщика данных" будет иметь формат json, так как для разных источников информации потребуется сохранять информацию в различном виде и сложно определить наиболее подходящий формат заранее.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица источники\label{tbl:sources}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                  | *Тип*      | *Описание*                                                         |
| Идентификатор             | Целое    | Уникальный идентификатор                                         |
| Сайт                      | Строка   | Сайт источника                                                   |
| Код типа источника        | Целое    | Внешний ключ из таблицы тип источника                            |
| Состояние сборщика данных | JSON     | Данные о текущем состояние сборщика данных, если возникнет сбой  |
| Дата последнего сбора     | DateTime | Точка когда сбор данных закончился, для дальнейшего сбора данных |

Аналогично для сфер компаний таблица для типов модели позволяет удобно работать с данными в дальнейшем.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица тип модели\label{tbl:model_type}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*    | *Описание*                 |
| Идентификатор   | Целое  | Уникальный идентификатор |
| Название модели | Строка |                          |

#+ATTR_LATEX: :environment longtblr :options caption={Таблица модели\label{tbl:model}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*    | *Описание*                           |
| Идентификатор   | Целое  | Уникальный идентификатор           |
| Название модели | Строка |                                    |
| Код типа модели | Целое  | Внешний ключ на таблицу тип модели |

#+ATTR_LATEX: :environment longtblr :options caption={Таблицы текст\label{tbl:text}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                | *Тип*      | *Описание*                          |
| Идентификатор           | Целое    | Уникальный идентификатор          |
| Ссылка                  | Строка   | Ссылка на текст                   |
| Код источника           | Целое    | Внешний ключ из таблицы источники |
| Дата текста             | DateTime | Время публикации текста           |
| Заголовок               | Строка   | Заголовок текста                  |
| Код компании            | Целое    | Внешний ключ на компанию          |
| Количество комментариев | Целое    |                                   |

Так как Bert на вход принимает отдельные предложения, было решено сделать для них отдельную таблицу.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица предложений\label{tbl:sentence}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*          | *Тип*    | *Описание*                              |
| Идентификатор     | Целое  | Уникальный идентификатор              |
| Код текста        | Целое  | Внешний ключ из таблицы тексты        |
| Предложение       | Строка |                                       |
| Номер предложения | Целое  | Порядковый номер предложения в тексте |

Так как результат работы модели может отличать в зависимости от ее типа, то поле "результат" будет массивом.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица результатов анализа текстов\label{tbl:text_result}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*                 | *Назначение*                                    |
| Идентификатор   | Целое               | Уникальный идентификатор                      |
| Код предложения | Целое               | Внешний ключ из таблицы предложения           |
| Код модели      | Целое               | Внешний ключ из таблицы модели                |
| Результат       | Вещественный массив | Результат работы модели                       |
| Обработано      | Логическое          | Показатель, обработано ли предложение или нет |

Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database]].
*** Проектирование базы данных для агрегации
При сборе функциональных требований было выявлено, что надо быстро показывать количество собранных отзывов и индекс компаний.

Обработанные данные из таблицы\nbsp{}\ref{tbl:text_result} агрегируются для каждого квартала и рассчитываются по формуле [[ref:eq:ethics]].
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для расчета и показа индекса\label{tbl:index_calc}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*          | *Описание*                                     |
| Идентификатор   | Целое        | Уникальный идентификатор                     |
| Год             | Целое        | Год за который был агрегирован индекс        |
| Квартал         | Целое        | Квартал за который был агрегирован индекс    |
| Название модели | Строка       |                                              |
| Сайт источника  | Строка       |                                              |
| Тип источника   | Строка       |                                              |
| Название банка  | Строка       |                                              |
| Код банка       | Целое        | Для запросов через API                       |
| Нейтральный     | Целое        | Количество нейтральных предложений за период |
| Позитивный      | Целое        | Количество позитивных предложений за период  |
| Негативный      | Целое        | Количество негативных предложений за период  |
| Базовый индекс  | Вещественное | Индекс для расчета итогового индекса         |
| Средний индекс  | Вещественное | Индекс для расчета итогового индекса         |
| Std индекс      | Вещественное | Индекс для расчета итогового индекса         |
| Индекс          | Вещественное | Рассчитанный индекс                          |

Собранные отзывы из таблицы\nbsp{}\ref{tbl:text} агрегируются для каждого месяца и рассчитывается количество собранных отзывов за месяц.
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для расчета и показа индекса\label{tbl:index_calc}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*           | *Тип*      | *Описание*                                  |
| Идентификатор      | Целое    | Уникальный идентификатор                  |
| Дата               | DateTime |                                           |
| Квартал            | Целое    | Квартал за который был агрегирован индекс |
| Тип источника      | Строка   |                                           |
| Сайт               | Строка   |                                           |
| Количество отзывов | Целое    |                                           |

Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_views]].
** Проектирование модуля работы с данными
Модуль будет представлять собой HTTP API для работой с базой данных.

Для работы с базой данных будут созданы классы, которые представляют ORM-модель для работы с базой данных.

При первом старте приложение будет получаться список компаний (банки, брокеры, микрокредитные организации и страховые) с сайта "Центрального банка России" и помещаться в базу данных. Из этих данных будет собираться лицензия компании и название компании, для микрокредитных организаций дополнительно будет собираться основной государственный регистрационный номер (ОГРН), так как под одной лицензией может работать несколько компаний. При последующих стартах приложение будет проверяться, что в каждом списке есть компании и новые компании не будут выгружаться.

Далее создаются объекты класса Bank с использованием полученных данных и добавляются в список cbr_banks. Наконец, список cbr_banks возвращается как результат работы функции.

Таким образом, принцип работы данного алгоритма заключается в извлечении необходимых данных из HTML-кода веб-страницы и преобразовании их в объекты класса Bank, что позволяет автоматизировать процесс получения и анализа информации о банках. Диаграмма классов рис.\nbsp{}[[ref:fig:cbr_parser_class]].

#+begin_src d2 :exports results :file img/d2/cbr_parser_class.png
BaseParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list()
  get_dataframe(url str, skip_rows int = 3, index_col str \| int \| None): "pd.DataFrame | None"
}

BankiParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

BrokerParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

InsuarenceParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

MfoParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

BaseParser -> BankiParser
BaseParser -> InsuarenceParser
BaseParser -> BrokerParser
BaseParser -> MfoParser
#+end_src

#+CAPTION: Диаграмма классов для сбора данных с сайта ЦБ
#+NAME: fig:cbr_parser_class
#+ATTR_LATEX: :placement [h!] :width \textwidth
#+RESULTS:
[[file:img/d2/cbr_parser_class.png]]

Для работы с источниками текстов необходимо сделать запросы для типов источников и самих источников. Также для обновления состояния сборщика данных надо сделать отдельный метод =PATCH=, который позволит обновлять время и состояние источника данных по идентификатору. Также при создании источника будет проверяться существует ли такой тип источника или нет. Если его не существует, то такой тип будет создаваться.

Сохранение текстов будет доступно по методу =POST= c передачей данных о тексте и состоянии сборщика данных. При выполнении запроса должно обновляться состояние сборщика данных, а каждый текст должен сохраняться, как набор предложений. При получении предложений должны выбираться такие предложения, которые еще не обработаны моделью.

Работа с моделями будет происходить аналогично источникам. При сохранении модели будет проверяться есть ли такой тип модели или нет. Если его нет, то он будет создан.

Также необходима возможность получения списка компаний с помощью API по различным сферам работы.

В результате проектирования должно получиться API, которое реализует запросы представленные в таблице @@latex:~\ref{tbl:api_doc}@@.

#+LATEX: \include{api_table}
** Проектирование модуля агрегации данных
Для построения индекса этичности компаний будет ежедневно агрегироваться база данных и перестраиваться индексы.
** Проектирование модуля сбора данных
У всех сборщиков данных одинаковый принцип работы (рис.\nbsp{}[[ref:fig:parser_flow]]):
1. Сборщик данных запрашивает у модуля работы с базой данных список сохраненных компаний. Модуль отвечает на запрос, отправляя список сохраненных компаний обратно.
2. Сборщик данных запрашивает у сайта для сбора данных список компаний на сайте. Сайт отправляет список компаний обратно в сборщик данных.
3. После получения списка компаний, сборщик данных сохраняет только те компании, которые уже есть в основной базе данных. Это делается для того, чтобы связать компании которые представлены на сайте и в базе данных.
4. Затем, сборщик данных начинает собирать данные для каждой компании из списка. Это может быть сделано путем отправки запросов к API сайта или сканирования страниц сайта для поиска нужных данных. Собранные данные затем сохраняются в основной базе данных. Сбор данных будет происходить до тех пор пока не соберутся все отзывы для компании, или дата отзыва дойдет до даты предыдущего сбора данных.

Для реализации сборщиков данных было решено сделать базовый класс, который представляет собой интерфейс с функцией =parse=. Из него наследуются интерфейсы для сбора данных для каждого сайта (banki.ru, sravni.ru, vk.com). Диаграмма классов рис.\nbsp{}[[ref:fig:parser_class_diagram]]. От этих базовых классов для каждого сайта будут наследоваться классы, которые собирают отзывы компаний из различных сфер. Было выбрано такое решение, так как представление информации в рамках одного сайта в различных разделах может сильно различаться. Также у каждого сборщика данных будет своя база данных для сохранения информации о компаниях.

#+begin_src mermaid :exports results :file img/mermaid/parser_flow.png
%%{
   init: {
     "theme": 'base',
     "themeVariables": {
       "primaryColor": '#FFF',
       "primaryTextColor": '#000',
       "primaryBorderColor": '#000',
       "lineColor": '#FFF'
     }
   }
}%%

sequenceDiagram
    participant A as Сборщик данных
    participant B as Сайт для сбора данных
    participant API as Модуль работы с данными
    participant DB as База данных<br/>сборщика данных
    A->>API: Получить список<br/>сохраненных компаний
    API->>A: Список сохраненных компаний
    A->>B: Получить список компаний на сайте
    B->>A: Список компаний с сайта
    A->>DB: Сохранение компаний,<br/>которые есть на сайте и в основной БД
    A->>B: Отправка собранных данных
#+end_src

#+CAPTION: Схема работы сборщиков данных
#+NAME: fig:parser_flow
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/mermaid/parser_flow.png]]

*** Проектирование сбора данных с banki.ru
Для получения данных с сайта banki.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных. Исходя из требований получилась база данных @@latex:~\ref{tbl:banki_ru}@@. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_banki_ru]].

#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта banki.ru\label{tbl:banki_ru}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*            | *Тип*    | *Описание*                                   |
| Идентификатор       | Целое  | Уникальный идентификатор                   |
| Идентификатор банка | Целое  | Идентификатор банка в основной базе данных |
| Имя банка           | Строка |                                            |
| Код банка           | Строка | Код банка для запросов по API              |

С этого сайта будут собираться данные о компаниях из пяти сфер:
1. *Отзывы на банки.*
   Список банков будет получаться из [[https://www.banki.ru/widget/ajax/bank_list.json]]. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.banki.ru/services/responses/list/ajax/]] и в параметры ссылки будет передаваться код банка и номер страницы с отзывами и из полученного json будут собираться данные об отзывах.
2. *Новости о банках.*
   В качестве списка компаний будет использоваться такой же список, как и для банков. Для получения текста новостей сначала будет собираться список новостей для компании. Для этого будут отправляться запросы на [[https://www.banki.ru/banks/bank/{bank.bank_code}/news/]] в зависимости от банка. Затем по каждой ссылке будет обрабатываться html код страницы и собираться текст новости.
3. *Отзывы на страховые компании.*
   Список компаний будет получаться из [[https://www.banki.ru/insurance/companies/]]. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/insurance/companies/]]. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
4. *Отзывы на брокеров.*
   Для получения списка компаний данные будут браться из [[https://www.banki.ru/investment/brokers/list/]]. Затем они будут сравниваться по номеру лицензии с брокерами, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/investment/responses/company/broker/]]. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
5. *Отзывы на микрокредитные организации.*
   Для получения списка компаний данные будут браться из [[https://www.banki.ru/microloans/ajax/search]]. Затем они будут сравниваться по номеру лицензии и ОГРН с компания, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/microloans/responses/ajax/responses/]]. Затем из полученного json собираются отзывы о компании.
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
*** Проектирование сбора данных с sravni.ru
Для получения данных с сайта sravni.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных, также для некоторых запросов надо иметь псевдоним компании (alias). Исходя из требований получилась база данных@@latex:~\ref{tbl:sravni_ru}@@. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_sravni_ru]].

#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта sravni.ru\label{tbl:sravni_ru}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*                     | *Тип*    | *Описание*                                   |
| Идентификатор                | Целое  | Уникальный идентификатор                   |
| Идентификатор банка          | Целое  | Идентификатор банка в основной базе данных |
| Код банка в sravni.ru        | Целое  |                                            |
| Старый код банка в sravni.ru | Целое  |                                            |
| Псевдоним компании           | Строка |                                            |
| Название банка               | Строка |                                            |
Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_sravni_ru]]

С этого сайта будут собираться данные о компаниях из трех сфер:
1. *Отзывы на банки.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =bank=. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/bank/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
2. *Отзывы на страховые компании.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =insuranceCompany=. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/strakhovaja-kompanija/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним страховой и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
3. *Отзывы на микрокредитные организации.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =mfo=. Затем они будут сравниваться по номеру лицензии и ОГРН с компаниями, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/zaimy/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
*** Проектирование сбора данных с vk.com
Для получения на сайт vk.com будут отправляться запросы на их API. Для этого предварительно будут собраны данные о всех организациях, которые у них представлены на сайте и перемещены в базу данных \ref{tbl:vk_com}. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_vk_com]].
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта vk.com\label{tbl:vk_com}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*                 | *Тип*    | *Описание*                 |
| Идентификатор            | Целое  | Уникальный идентификатор |
| Идентификатор на vk.com  | Строка |                          |
| Имя компании             | Строка |                          |
| Домен компании на vk.com | Строка |                          |

Для доступа к API будет зарегистрировано приложение для получения ключа к нему. Для каждой компании будут выгружаться посты пока дата последней выгрузки не более чем дата последнего поста для этого будет отправляться запрос на [[https://api.vk.com/method/wall.get]], куда будет подставляться токен приложения и идентификатор группы. Затем для каждого поста будут выгружаться комментарии по методу [[https://api.vk.com/method/wall.getComments]], а затем отправляться в модуль работы с базой данных.
** Проектирование модуля обработки данных
Модуль обработки данных будет представлять собой дообученную нейронную сеть Sentence-BERT. В качестве основы для обучения будет RuBERT\nbsp{}[cite:@kuratov_adaptation_2019]. При использовании BERT для задачи классификации, добавляется небольшой слой нейронной сети в конце предобученной модели, который выполняет финальную классификацию. Этот слой называется "головой классификации" (classification head). Голова классификации содержит несколько слоев нейронной сети, которые принимают входные векторы, выходные значения которых интерпретируются как вероятности принадлежности к различным классам. Количество выходных нейронов в голове классификации равно количеству классов в вашей задаче. При дообучении BERT для задачи классификации, веса всех слоев в предобученной модели остаются неизменными, а только голова классификации обучается на задаче классификации с использованием обучающей выборки. Таким образом, голова классификации добавляется к BERT при задаче классификации, и обучается на конкретной задаче классификации, используя представления слов, полученные от предобученной модели BERT.

В этой работе для дообучения будет использоваться набор из 20,000 предложений, размеченный экспертами на соответствие этическим практикам. Так как классы предложений получились не сбалансированными\nbsp{}[[ref:fig:class_balance]] (отрицательных предложений больше, на диаграмме обозначены как "-", чем положительных "+"), то в качестве основной метрики будет использоваться F1.

#+CAPTION: Распределение классов
#+NAME: fig:class_balance
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/class_balance.png]]
** Выводы по главе
В данной главе были представлены результаты проектирования системы и ее отдельных компонентов и их взаимодействие, включая базы данных и API микросервисов, согласно выявленным требованиям из первой главы. Каждый микросервис был разработан с учетом принципов микросервисной архитектуры и обеспечивает определенную функциональность, необходимую для реализации системы в целом.

Была спроектирована база данных для хранения информации об отзывах, источниках, моделях и компаниях. Базы данных были спроектированы с учетом требований к масштабируемости и производительности системы.

Также были спроектированы сервисы для работы с базой данных, ее агрегацией, сбором данных и обработки данных.

Эти результаты будут использоваться при разработке и реализации системы в следующих этапах проекта.
* Реализация системы
В данной главе описывается реализация системы и каждого микросервиса, обучение модели.

Этап реализации можно разделить на пункты:
1. Реализация базы данных.
2. Реализации модулей для собора, работы и агрегации данных.
3. Обучение модели и реализация модуля обработки данных.
4. Развертывание системы.
** Реализация базы данных
Для хранения информации в системе была выбрана СУБД PostgreSQL. Для создания базы данных был выбран подход "code first", который позволяет определить структуру базы данных в виде классов на языке Python. Для этого использовалась библиотека Sqlalchemy\nbsp{}[cite:@bayermichael_architecture_2012], которая обеспечивает ORM-модель для работы с базами данных. При запуске приложения база данных будет создаваться автоматически на основе определенных классов.

Для определения структуры базы данных был создан базовый класс =DeclarativeBase=, который является родительским для всех классов, определяющих таблицы базы данных. Каждая таблица базы данных определяется в виде отдельного класса, который наследует базовый класс и содержит определения столбцов и связей между таблицами.

Для обеспечения возможности модернизации базы данных в дальнейшем была использована библиотека alembic, которая обеспечивает миграции базы данных и позволяет вносить изменения в структуру базы данных без потери данных.
** Реализация модуля работы с базой данных
Для реализации API используется асинхронный фреймворк FastAPI и для взаимодействии с базой данных асинхронная библиотека asyncpg. Для валидации приходящих данных и ответов для каждого запроса была создана своя модель с помощью библиотеки Pydantic. Также с помощью Pydantic был сделан класс для получения строки подключения к базе данных из переменных окружения.

При старте приложения сначала проверятся подключение с базой данных и проверяется ее версия, если она не актуальна, то выполняются миграции для ее актуализации. Затем проверятся список компаний, если список компаний пустой, то собирается данные о банках, брокера, страховых и микрофинансовых организациях.

Информация о банках будет собираться по ссылке [[https://www.cbr.ru/banking_sector/credit/FullCoList/]]. Алгоритм начинается с получения объекта BeautifulSoup\nbsp{}[cite:@richardsonleonard_beautiful_2007], который содержит HTML-код веб-страницы. Затем происходит итерация по всем элементам таблицы, начиная со второй строки, так как в первой находится заголовки для каждой колонки. Для каждой строки таблицы находятся все ячейки, извлекаются регистрационный номер (номер лицензии) и название банка. В списке также есть платежные небанковские кредитные организации, которые имеют буквы на конце лицензии, например =3511-К= у "Деньги.Мэйл.Ру". Для этого такие номера будут разделяться по "-" и браться номер и преобразовываться в число. Затем собранные данные помещаются в базу данных.

Для сбора данных о брокерах будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_brokers.xlsx]], с помощью библиотеки pandas\nbsp{}[cite:@team_pandas-devpandas_2023]. При запуске происходит загрузка таблицы с данными о брокерах в формате Excel, после чего данные из таблицы считываются. Затем происходит итерация по строкам таблицы и для каждой строки создается экземпляр класса Bank, который содержит информацию о банке-брокере, такую как номер лицензии, наименование организации и тип банка. Для удобства хранения номера лицензии, из них удалялись все знаки "-".

Для сбора данных о страховых будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_ssd.xlsx]]. Так как в файле много строк, которые не содержат номеров или наименований банков, то они удаляются из него. Номера лицензий хранятся в формате =СИ № 3847= или =ОС № 1083 - 05= и для получения номера берется первое число которое встретилось в строке с помощью регулярного выражения. Затем полученная информация помещается в базе данных.

Для сбора данных о микрофинансовых организациях будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_ssd.xlsx]]. В этом файле номер лицензии разбит по 5 ячейкам и в части из отсутствуют числа. Поэтому отсутствующие ячейки заполняются нулями и содержание ячеек объединяется для получения результата. Потом также берется название компании и эта информация помещается в базу данных.

API было реализовано согласно требованиям описанными во второй главе.

Алгоритм получения предложений для обработки проверяет, какие из них уже были обработаны моделью, а какие - нет. Если для каждого запроса искать пересечение множества предложений, которые еще не обработаны моделью и уже обработаны, это может занять много времени. Поэтому сначала выполняется запрос\nbsp{}([[ref:lst:insert_unused]]), который ищет предложения, еще не обработанные моделью. Если таких нет, то в таблицу с результатами добавляются 100 000 предложений с пустыми результатами, чтобы было проще искать предложения при дальнейших запросах. Затем с помощью запроса\nbsp{}([[ref:lst:select_unused]]) из таблицы с результатами выбираются предложения, еще не обработанные моделью. Ниже приведены SQL запросы, которые генерирует ORM.

#+NAME: lst:insert_unused
#+CAPTION: SQL запрос на вставку не обработанных предложений
#+begin_src sql
INSERT INTO text_result (text_sentence_id, model_id, is_processed)
SELECT text_sentence.id, :model_id, false
FROM text_sentence
JOIN text ON text_result.text_id = text.id
JOIN source ON text.source_id = source.id
LEFT JOIN (
  SELECT text_result.text_sentence_id
  FROM text_result
  WHERE text_result.model_id = :model_id
) AS subq ON text_sentence.id = subq.text_sentence_id
WHERE source.site IN (:sources) AND subq.text_sentence_id IS NULL
LIMIT 100000;
#+end_src

#+NAME: lst:select_unused
#+CAPTION: SQL запрос на получение еще не обработанных предложений
#+begin_src sql
SELECT text_sentence.id, text_sentence.sentence
FROM text_sentence
JOIN (
  SELECT text_result.text_sentence_id, text_result.id
  FROM text_result
  WHERE text_result.model_id = :model_id AND text_result.is_processed = false
  LIMIT :limit
) AS sub
ON text_sentence.id = sub.text_sentence_id;
#+end_src

Для разделение текста на предложения при получении текста используется библиотека =nltk=\nbsp{}[cite:@bird_natural_2009].

Для валидации параметров отвечающих за тип индекса этичности, список источников и период агрегации для получения агрегированных данных были сделаны =Enum=-классы. Если в запрос для получения статистики был передан параметр показывающий, что надо агрегировать только по годам, то в запрос подставлялась дополнительная часть с =group by=.

Для получения данных об обработанных предложения в зависимости от типа запрашиваемого индекса в запрос подставлялся нужный тип индекса и проводилась агрегация данных аналогично запросу на получение статистики.
** Реализация модуля агрегации данных
Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции.

При запуске модуля начинается подсчет количества собранных отзывов и расчет индекса этичности в разных потоках.

Так как в базе данных находится очень много элементов, то было решено обновлять данные напрямую из SQL. Код запроса на расчет статистки\nbsp{}[[ref:lst:count_reviews]].

#+NAME: lst:count_reviews
#+CAPTION: SQL запрос на подсчет количества предложений
#+begin_src sql
INSERT INTO text_sentence_count (count_reviews, date, quarter, source_type, source_site)
SELECT COUNT(text.id) AS reviews_count,
       DATE_TRUNC('month', text.date) AS month,
       EXTRACT('quarter' FROM text.date) AS quarter,
       source_type.name AS source_type,
       source.site AS source_site
FROM text
JOIN source ON text.source_id = source.id
JOIN source_type ON source.source_type_id = source_type.id
GROUP BY month, quarter, source.site, source_type.name;
#+end_src

Запрос для создания запроса\nbsp{}[[ref:lst:sql_aggregate]] на расчет данных было решено использовать несколько подзапросов:
1. Сначала рассчитывается логарифм результата обработки предложений для каждой колонки. Для избежания проблем с логарифмами к каждому значению добавляется маленькое число, так как у некоторые значения могут быть нулевыми. Этот подзапрос создан для того, чтобы ускорить выполнение, так как этот расчет можно было объединить со следующим подзапросом, но из-за этого пришлось бы пересчитывать одинаковые значения несколько раз.
2. Затем для подсчета предложений разных типов определяется их категория. Для этого используется конструкция =case when=, где значение обработанных категорий сравнивается попарно.
3. Потом к полученным данным присоединяются данные из других таблиц. Извлекается информация о квартале и дате, значения с предыдущего шага суммируются. Сам запрос объединяется для каждого квартала компаний, для каждого источника отдельно.
4. И в конце полученные данные вставляются в таблицу.
5. Затем уже на агрегированных данных рассчитываются значение индекса согласно формуле\nbsp{}[[ref:eq:ethics]].

#+name: lst:sql_aggregate
#+caption: SQL запрос на агрегацию обработанных предложений
#+begin_src sql
INSERT INTO aggregate_table_model_result (bank_id, bank_name, quater, year, model_name, source_site, source_type, positive, neutral, negative, total)
SELECT
    extract(year from text.date) as year,
    extract(QUARTER from text.date) as quarter,
    bank.id as "bank_id",
    model.name as "model_name",
    source.site as "source_site",
    source_type.name as "source_type_name",
    sum(positive) as "positive",
    sum(neutral) as "neutral",
    sum(negative) as "negative",
    sum(positive+neutral+negative) as total
FROM
    (SELECT
        text_sentence_id,
        model_id,
        case when (log_positive > log_neutral) and (log_positive > log_negative) then 1 else 0 end as "positive",
        case when (log_neutral  > log_positive) and (log_neutral > log_negative) then 1 else 0 end as "neutral",
        case when (log_negative > log_neutral) and (log_negative > log_positive) then 1 else 0 end as "negative"
    FROM (
        SELECT
            text_sentence_id,
            model_id,
            (LOG(result[1]+0.0000001)) as "log_neutral",
            (LOG(result[2]+0.0000001)) as "log_positive",
            (LOG(result[3]+0.0000001)) as "log_negative"
        FROM text_result
        WHERE model_id = 1) t) pos_neut_neg
JOIN
    text_sentence ON pos_neut_neg.text_sentence_id = text_sentence.id
JOIN
    text ON text_sentence.text_id = text.id
JOIN
    bank ON text.bank_id = bank.id
JOIN
    source ON source.id = text.source_id
JOIN
    source_type ON source.source_type_id = source_type.id
JOIN
    model ON model.id = pos_neut_neg.model_ida
GROUP BY quarter, year, source.site, source_type.name, bank.id, model.name
#+end_src
** Реализация модуля сбора данных

Для каждого сайта будет создана отдельная папка (модуль) со схожей структурой:

Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции, для обработки html страниц используется библиотека BeautifulSoup, также для обработки данных используется библиотека Pydantic.
1. В файле =database= будет лежать схема модели базы данных.
2. =schemes= pydantic модели для обработки текста.
3. =queries= запросы в базу данных.

Также для всех сборщиков данных была выделена общая часть, включающая модуль запросов, модулей объектов и настроек, а также модуль для запросов к базе данных. Модуль запросов является модификацией библиотеки requests\nbsp{}[cite:@chandra_python_2015] и предоставляет возможность повторного выполнения запросов в случае неудачи и обработки формата json. Модуль моделей содержит pydantic классы объектов для работы с запросами к базе данных и обработки данных. Модуль настроек представляет pydantic класс, который получает данные о подключении к базе данных, ссылке на API и токен для работы с API ВКонтаке из окружения приложения. Модуль для запросов к API предоставляет набор функций для выполнения запросов.

Для удобства развертывания было решено запускать сборщик данных в зависимости от аргумента с которым запущен код. Потом при запуске в зависимости от переданных аргументов создается база данных и запускается сборщик. Процесс сбора данных запускается ежедневно с помощью библиотеки schedule.

*** Разработка модуля сбора данных с banki.ru
Сбор данных с bani.ru осложнен тем, что компании из разных сфер имеют разное представление на сайте, поэтому для каждой сферы нужен свой подход. Также стоит отметить, что для успешной отправки запросов на сайт, требуется в заголовках запроса добавлять параметр "X-Requested-With" со значением "XMLHttpRequest".

Для сбора данных был создан базовый класс, который реализует главный цикл сбора дынных. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний с сайта и проверяется какие компании уже есть в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика. Для этого каждый класс должен будет реализовать функцию для получения списка компаний =load_bank_list=. Затем запускается сбор данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Далее берется количество страниц отзывов у компании. Потом для каждой компании берутся тексты с помощью функции =get_page_bank_reviews= и сохраняются тексты, которые не были еще собраны. Затем полученные тексты в модуль работы с базой данных.

Как реализована функция =load_bank_list= для различных сфер:
1. *Банки*. Для получения этого списка компаний будет отправляться запрос по адресу [[https://www.banki.ru/widget/ajax/bank_list.json]] и из полученного json собираться список компаний.
2. *Страховые*. Для получения списка компаний сначала загружается html страница со списком по адресу [[https://www.banki.ru/insurance/companies/]]. Затем в ищется элемент div с атрибутом =data-module= равным =ui.pagination=. Из этого элемента из атрибута =data-options= получается количество компаний и страниц с ними. Потом для каждой страницы с компаниями ищутся все элементы =tr= с атрибутом =data-test= равным =list-row=. Из этого элемента получается вся информация о компании. Потом полученные компании сравниваются с теми, что сохранены в основной базе данных и сохраняются в базу сборщика данных.
3. *Брокеры*. Для получения списка компаний отправляется запрос по адресу [[https://www.banki.ru/investment/brokers/list/]], но без дополнительного заголовка, так как только без него появляются лицензии компаний. Потом из этого списка собирается информация о компаниях и сохраняется в базе данных.
4. *Микрофинансовые организации*. Для получения списка компаний отправляется запрос по адресу [[https://www.banki.ru/microloans/ajax/search]]. Сначала из полученного json получается количество страниц с компаниями. Затем для каждой страницы отправляются новые запросы и обрабатывается информация о компаниях. Потом собранные компания сравниваются с компаниями из основной базы данных по номеру лицензии и ОГРН с компаниями из основной базы данных и сохраняются в базе данных сборщика данных.

Реализация функции =get_page_bank_reviews= для различных сфер:
1. *Банки*. Для получения отзывов будет делаться запрос по адресу [[https://www.banki.ru/services/responses/list/ajax/]] с параметрами для определения компании и номера страницы. Из полученного json соберутся отзывы и отправятся в основную базу данных.
2. *Новости*. Для получения текстов новостей сначала будут собираться адреса новостей, а затем уже сами тексты новостей. Для сбора адресов будет отправляться запрос на [[https://www.banki.ru/banks/bank/{bank_code}/news/]], где "=bank_code=" код банка, также в качестве параметра запроса будет отправляться номер страницы. Для получения адресов будут браться элементы "a" с классом "text-list-link", также для отбора новых новостей будут обрабатываться даты. Для этого будут браться элементы "span" с классом "text-list-date". Потом по полученным ссылкам будет браться html код страниц и браться текст новости из элементов "p".
4. *Страховые и Брокеры*. Для этих сфер тексты отзывов получаются путем обработки html страниц. Они получаются из запросов на адреса [[https://www.banki.ru/investment/responses/company/broker/]] для брокеров и [[https://www.banki.ru/insurance/responses/company/]] для страховых, к этим ссылкам добавляется код компании и номер страницы для получения отзывов. Потом для получения текста отзывов ищутся элементы "div" с классом "=responses__item__message=" и из него берется текст. Затем собранные отзывы отправляются в модуль работой с базой данных.
6. *Микрофинансовые организации*. Для получения отзывов отправляются запросы на [[https://www.banki.ru/microloans/responses/ajax/responses]], где в параметры передаются код компании и номер страницы. Затем из полученного json собираются отзывы и отправляются в основную базу данных.

*** Разработка модуля сбора данных с sravni.ru
При сборе данных со sravni.ru будут отправляться запросы на их внутреннее API, которое имеет схожую структуру для всех сфер компаний. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний. Он будет получать путем отправки запроса на [[https://www.sravni.ru/proxy-organizations/organizations]] с различным значением параметра =organizationType= ("bank" для банков, "insuranceCompany" для страховых компаний и "microcredits" для микрофинансовых организаций). Потом полученный список компаний проверяется со списком, который сохранен в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика.

Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Потом для каждой компании получается список отзывов. Он получается путем отправки запроса по адресу [[https://www.sravni.ru/proxy-reviews/reviews]] с параметром "reviewObjectType" с такими же значениями, как для получения списка компаний, и идентификатором компании на сайте sravni.ru. В результате запроса получается json, в котором находится 1000 отзывов на компанию. Из этих отзывов выбираются новые отзывы с момента предыдущего сбора данных. Потом собранные данные отправляются в основную базу данных.
*** Разработка модуля сбора данных с vk.com
Для взаимодействия с API ВКонтакте был реализован класс, который делает запросы к API и подставляет обязательные параметры, такие как токен и версия API, так и параметры которые нужны для различных методов. Также этот класс регулирует количество запросов к API, так как разрешено делать не более трех запросов в секунду.

При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список отобранных заранее компаний. Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Затем для каждой компании берет публикации в группе. Для публикаций у которых разница во времени с момента предыдущего сбора данных не более недели собираются новые комментарии. Из собранных комментариев удаляются эмоджи и идентификаторы пользователей из ссылок на профиля ВКонтакте, которые имеют вид =(ID пользователя|Имя пользователя)=. Потом собранные комментарии отправляются в модуль работы с базой данных.
** Реализация модуля обработки текста
За основу была взята модель от DeepPavlov =rubert-base-cased=\nbsp{}[cite:@kuratov_adaptation_2019]. К этой модели был добавлен слой для классификации предложений на 3 категории. В качестве обучающей выборки использовался использоваться набор из 20,000 предложений, размеченный экспертами на соответствие этическим практикам. Для обучения модели замораживались все ее слои, кроме двух последних.

Для удобной работы с датасетом был сделан класс =pytorch.Dataset=, который на каждой итерации модели берет новые предложения и обрабатывает (токенизирует) их для подачи модели. Классы из собранных данных ("?", "+", "-") конвертировались в 0, 1, 2. Полученные данные разделялись на обучающую и тестовую выборку в отношении 8 к 2.

Для обучения модели использовался оптимизатор =Adam=\nbsp{}[cite:@kingma_adam_2017] с параметром обучения 0.001. Потом для каждой эпохи обучения выполняются следующие действия:
1. Для обучения модели:
   1. Устанавливается модель в режим обучения.
   2. Вычисляется функция потерь на каждом батче из обучающей выборки, и производится обновление весов модели с помощью выбранного оптимизатора.

2. Для тестирования модели:
   1. Модель переводится в режим оценки.
   2. Для каждого батча из тестовой выборки вычисляются предсказания модели и сохраняются.
   3. Вычисляется f1 для тестовой выборки.

#+CAPTION: Результаты F1 на тестовой выборке
#+NAME: fig:test_f1
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/test_f1.png]]

#+CAPTION: Средняя ошибка модели на каждой эпохе
#+NAME: fig:model_loss_epoch
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/model_loss_epoch.png]]

Потом обученная модель сохраняется и периодически запускается и обрабатывает собранные данные. Она получает их делая запросы к API, а затем отправляет результаты обработки обратно.
** Развертывание системы
Для развертывания системы каждый компонент был выделен в отдельный контейнер docker\nbsp{}[cite:@merkel_docker_2014], а для оркестрации приложений существует инструмент Docker Compose, который позволяет запускать многоконтейнерные приложения с помощью YAML-файлов конфигурации. Для установки переменных окружения в контейнеры используется файл ".env", в котором содержались переменные окружения для всех приложений и путь к этому файлу прописывался в параметрах =env_file=. Для установки в названия базы данных дополнительно в конфигурации контейнеров указывалась переменная окружения =POSTGRES_DB=.

Для реализации модуля взаимодействия с базой данной в качестве базового образа использовался =python:3.10=. Потом устанавливалась библиотека nltk и данные для работы этой библиотеки с русским языком. Затем устанавливались зависимости приложения и оно запускалось с помощью библиотеки uvicorn. Для модулей агрегации базы и сбора данных в качестве базового образа использовался =python:3.10-slim=, так как он использует меньше памяти, чем обычный.

В данной работе определен сервис "database", который запускает контейнер с PostgreSQL версии 14.4. Контейнеру также присваиваются тома для хранения данных, которые будут использоваться внутри контейнера. Для обеспечения доступности сервиса в контейнере определены порты, через которые можно подключаться к базе данных. Кроме того, в конфигурации определен healthcheck, который проверяет работоспособность сервиса, выполняя команду =pg_isready= с заданными параметрами. Этот healthcheck запускается каждые 10 секунд и проходит 5 попыток, если проверка не прошла в заданный таймаут в 5 секунд. Такой подход обеспечивает более стабильную работу контейнера и позволяет оперативно реагировать на возможные проблемы.

Для модуля работы с базой данных определен сервис "api", который зависит от сервиса базы данных и начнет работу только после того, как у базы данных пройдет healthcheck.

Для модуля сбора для каждого сайта создавался новый контейнер и они запускались с помощью команды =python main.py --site name=, где вместо name было название сайта и из какой сферы собирались тексты. Все сборщики данных были выделены в отдельный профиль для удобства запуска.
** Выводы по главе
В данной главе представлена реализация системы и ее отдельных компонентов, включая базы данных и API микросервисов, согласно выявленным требованиям из первой главы. Каждый микросервис был разработан с учетом принципов микросервисной архитектуры и обеспечивает определенную функциональность, необходимую для реализации системы в целом, согласно проектированию описанном в предыдущей главе.

В результате было собрано (рис.\nbsp{}[[ref:fig:collected_data]]) 10 миллионов предложений для разных компаний и они были обработаны с помощью разработанной модели (рис.\nbsp{}[[ref:fig:ethics_analisys]]).

#+CAPTION: График собранных предложений
#+NAME: fig:collected_data
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/collected_data.png]]

#+CAPTION: График оценки этичности компаний
#+NAME: fig:ethics_analisys
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/ethics_analisys.png]]
* Тестирование системы
В данной главе описывается разные методы тестирования, которые были использованы при реализации системы и форматтеры кода и инструменты статического анализа также является важным, поскольку это позволяет снизить количество ошибок и повысить качество кода.
** Форматирование кода
Во время реализации системы использовались различные инструменты для форматирования кода и обеспечения его качества, такие как black, isort, pyupgrade, flake8 и mypy.

Black -- это инструмент для автоматического форматирования кода на Python. Он помогает унифицировать стиль кодирования и повысить читаемость кода. Он применяет определенный набор правил форматирования и применяет их к коду.

Isort -- это инструмент для автоматической сортировки импортов в Python-коде. Он обеспечивает единообразие сортировки импортов, что может повысить читаемость кода и уменьшить количество ошибок.

Pyupgrade -- это инструмент, который автоматически обновляет код Python до более новых версий языка, что позволяет использовать новые возможности языка и уменьшить количество устаревших функций и библиотек.

Flake8 -- это инструмент для обнаружения синтаксических и стилистических ошибок в коде Python. Он проверяет код на соответствие стандартам кодирования и выдает предупреждения, если обнаруживает ошибки.

Mypy -- это инструмент для статического анализа кода Python, который позволяет обнаруживать ошибки на этапе написания кода. Он проверяет типы переменных и аргументов функций и выдает ошибки, если они не соответствуют ожидаемым типам.

Также для автоматического форматирования кода использовался инструмент pre-commit, который позволяет форматировать кодовую базу перед отправкой в репозиторий с кодом.
** Тестирование системы
Для тестирования системы были написаны скрипты Github actions, которые запускали тестирование системы при отправки кода в репозиторий. Для тестирования использовалась библиотека pytest [cite:@krekel_pytest_2004], также для анализа покрытия кода использовалась библиотека =pytest-cov=.
*** Тестирование модуля работы с базой данных
При запуске каждого теста создавалась новая база данных и к ней применялись миграции. В зависимости от нужного теста в базу данных помещались нужные данные. Каждый метод API тестировался на верные данные, не верные данные и пустые данные, затем проверялось, что записалось в базу данных. В зависимости от требований каждого теста в базу данных помещались соответствующие данные. Затем создавался клиент для тестирования API с помощью библиотеки httpx.

Для каждого метода API тестировалось его поведение на правильные, неправильные и пустые данные. Затем проверялось, что записалось в базу данных и соответствует ожиданиям.

#+CAPTION: Покрытие кода тестами
#+NAME: fig:api_coverage
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/api_coverage.png]]
*** Тестирование модуля сбора данных
Тестирование сборщиков данных заключается в проверке корректности извлечения и обработки данных, получаемых из HTML-страниц и json. Также важно проверять сбор данных на актуальных данных сайтов. Для имитации запросов к API и сайтам использовалась библиотека requests-mock, для сохранения данных страниц использовалась библиотека =vcrpy=. Данная библиотека при первом запросе сайта сохраняет данные запроса (заголовки, параметры и так далее), а тело запроса кодирует в байты, что позволяет сократить объем. При последующих запросов вместо реального ответа будут подставляться данные предыдущего ответа. Такой способ работы с запросами позволяет ускорить процесс тестирования. Также при запуске каждого теста создавалась новая база данных.

При написании тестирования были реализованы mock-данные для API. Для подмены запросов на тестовые сначала делались запросы (или брались данные предыдущих запросов), затем с помощью библиотеки =requests-mock= данные запроса подменялись на тестовые. Потом запускались тесты, которые проверяли корректности работы функций.

#+CAPTION: Схема базы данных
#+NAME: fig:parser_coverage
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/parser_coverage.png]]
** Выводы по главе
В данной главе были описаны различные виды тестирования, которые использовались при тестировании модулей для работы базой данных и сбора данных.

Были описаны тесты для сборщиков данных, которые позволяют убедиться в корректности работы при различных сценариях. Также были рассмотрены тесты для HTTP API, которые проверяют работу API на правильную обработку запросов и на корректное взаимодействие с базой данных.

Важно отметить, что тестирование должно быть проведено на всех этапах разработки, начиная с написания кода и заканчивая релизом готового продукта. Это позволит выявить ошибки на ранних стадиях разработки и снизить риски возникновения критических проблем в работе системы в будущем.

Наконец, использование форматтеров кода и инструментов статического анализа позволяет снизить количество ошибок и повысить качество кода, что также важно для обеспечения корректной работы системы.
* Заключение
:PROPERTIES:
:UNNUMBERED: t
:END:
В данной работе была разработана система, которая анализирует этичность компаний на основании отзывов. Для этого были разработаны модули, которые позволяют работать с базой данных, собирать данные, агрегировать данные и обрабатывать их.

Для этого был проведен анализ литературы, разработаны модули и реализована модель.

Полученные результаты можно применять для оценки этичности компаний.

#+LATEX: \putbibliography
#+LATEX: \appendix
#+LATEX: \include{tz}
* Схема базы данных
#+begin_src d2 :exports results :file img/d2/database.png
bank: {
  shape: sql_table

  id: integer {constraint: primary_key}
  bank_name: varchar
  description: varchar
  bank_type_id: integer {constraint: foreign_key}
  licence: varchar
}

bank_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
}
model: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
  model_type_id: integer {constraint: foreign_key}
}
model_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  model_type: varchar
}
source: {
  shape: sql_table

  id: integer {constraint: primary_key}
  site: varchar
  source_type_id: integer {constraint: foreign_key}
  parser_state: JSON
  last_update: timestamp
}
source_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
}
text: {
  shape: sql_table

  id: integer {constraint: primary_key}
  link_: varchar
  source_id: integer {constraint: foreign_key}
  date: timestamp
  title: varchar
  bank_id: integer {constraint: foreign_key}
  comment_num: integer
}
text_result: {
  shape: sql_table

  id: integer {constraint: primary_key}
  text_sentence_id: integer {constraint: foreign_key}
  model_id: integer {constraint: foreign_key}
  result: double precision\[\]
  is_processed: boolean
}

text_sentence: {
  shape: sql_table

  id: integer {constraint: primary_key}
  text_id: integer {constraint: foreign_key}
  sentence: varchar
  sentence_num: integer
}

bank -> bank_type: bank_type_id
model -> model_type: model_type_id
source -> source_type: source_type_id
text -> bank: bank_id
text -> source: source_id
text_result -> model: model_id
text_result -> text_sentence: text_sentence_id
text_sentence -> text: text_id
#+end_src

#+CAPTION: Схема базы данных
#+NAME: fig:database
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/database.png]]

#+begin_src d2 :exports results :file img/d2/views_database.png
aggregate_table_model_result: {
  shape: sql_table

  id: integer {constraint: primary_key}
  year: integer
  quater: integer
  model_name: varchar
  source_site: varchar
  source_type: varchar
  bank_name: varchar
  neutral: integer
  positive: integer
  negative: integer
  total: integer
  bank_id: integer
  index_base: double precision
  index_mean: double precision
  index_std: double precision
  index_safe: double precision
  # index_base_10_percentile: double precision
  # index_base_90_percentile: double precision
  # index_mean_10_percentile: double precision
  # index_mean_90_percentile: double precision
  # index_std_10_percentile: double precision
  # index_std_90_percentile: double precision
  # index_safe_10_percentile: double precision
  # index_safe_90_percentile: double precision
}

text_reviews_count: {
  shape: sql_table

  id: integer {constraint: primary_key}
  date: timestamp
  quarter: integer
  source_site: varchar
  source_type: varchar
  count_reviews: integer
}
#+end_src

#+CAPTION: Схема базы данных для агрегаций
#+NAME: fig:database_views
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/views_database.png]]

#+begin_src d2 :exports results :file img/d2/banki_ru.png
banki\.ru: {
  shape: sql_table

  id: integer {constraint: primary_key}
  bank_id: integer
  bank_name: varchar
  bank_code: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта banki.ru
#+NAME: fig:database_banki_ru
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/banki_ru.png]]

#+begin_src d2 :exports results :file img/d2/sravni_ru.png
sravni\.ru: {
  shape: sql_table

  id: integer {constraint: primary_key}
  bank_id: integer
  sravni_id: integer
  sravni_old_id: integer
  alias: varchar
  bank_name: varchar
  bank_full_name: varchar
  bank_official_name: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта sravni.ru
#+NAME: fig:database_sravni_ru
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/sravni_ru.png]]

#+begin_src d2  :exports results :file img/d2/vk_com.png
vk\.com: {
  shape: sql_table

  id: integer {constraint: primary_key}
  vk_id: integer
  name: varchar
  domain: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта vk.com
#+NAME: fig:database_vk_com
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/vk_com.png]]

#+begin_src mermaid :exports none
classDiagram
direction BT
class aggregate_table_model_result {
   integer year
   integer quater
   varchar model_name
   varchar source_site
   varchar source_type
   varchar bank_name
   integer neutral
   integer positive
   integer negative
   integer total
   integer bank_id
   double precision index_base
   double precision index_mean
   double precision index_std
   double precision index_safe
   double precision index_base_10_percentile
   double precision index_base_90_percentile
   double precision index_mean_10_percentile
   double precision index_mean_90_percentile
   double precision index_std_10_percentile
   double precision index_std_90_percentile
   double precision index_safe_10_percentile
   double precision index_safe_90_percentile
   integer id
}
class alembic_version {
   varchar(32) version_num
}
class bank {
   varchar bank_name
   varchar description
   integer bank_type_id
   varchar licence
   integer id
}
class bank_type {
   varchar name
   integer id
}
class model {
   varchar name
   integer model_type_id
   integer id
}
class model_type {
   varchar model_type
   integer id
}
class source {
   varchar site
   integer source_type_id
   varchar parser_state
   timestamp last_update
   integer id
}
class source_type {
   varchar name
   integer id
}
class text {
   varchar link
   integer source_id
   timestamp date
   varchar title
   integer bank_id
   integer comment_num
   integer id
}
class text_result {
   integer text_sentence_id
   integer model_id
   double precision[] result
   boolean is_processed
   integer id
}
class text_reviews_count {
   timestamp date
   integer quarter
   varchar source_site
   varchar source_type
   integer count_reviews
   integer id
}
class text_sentence {
   integer text_id
   varchar sentence
   integer sentence_num
   integer id
}

aggregate_table_model_result  -->  bank : bank_id:id
bank  -->  bank_type : bank_type_id:id
model  -->  model_type : model_type_id:id
source  -->  source_type : source_type_id:id
text  -->  bank : bank_id:id
text  -->  source : source_id:id
text_result  -->  model : model_id:id
text_result  -->  text_sentence : text_sentence_id:id
text_sentence  -->  text : text_id:id
#+end_src

#+LATEX: \begin{landscape}
* Диаграмма классов
#+begin_src d2 :exports none :file img/d2/parser_class_v2.png
BaseParser: {
  shape: class

  parse()
  get_source_params(source Source): "tuple[int, int, datetime]"
}

BankiBase: {
  shape: class

  create_source(): Source
  load_bank_list()
  get_pages_num_html(url str, params dict\[str, Any\] \| None): "int | None"
  parse()
  get_pages_num(bank BankiRuBase): "int | None"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_reviews_from_url(url str, bank BankiRuBase, parsed_time datetime, params dict\[str, Any\] \| None): "list[Text]"
}

BankiBroker: {
  shape: class

  get_broker_licence_from_url(url str): "str | None"
  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BankiInsurance: {
  shape: class

  get_pages_num_insurance_list(url str): int
  load_bank_list()
  get_pages_num(bank BankiRuBase): "int | None"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
}

MfoParser: {
  shape: class

  get_mfo_json(page int): "dict[str, Any] | None"
  get_mfo_json_reviews(bank BankiRuBase, page int): "dict[str, Any] | None"
  json_total_pages(response dict\[str, Any\]): int
  get_microfin_list(): "list[BankiRuMfoScheme]"
  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BankiNewsParser: {
  shape: class

  get_pages_num(bank BankiRuBase): "int | None"
  bank_news_page(bank BankiRuBase, page int = 1): "BeautifulSoup | None"
  get_news_links(bank BankiRuBase, parsed_time datetime, page_num int = 1): "list[str]"
  news_from_links(bank BankiRuBase, news_urls list\[str\]): "list[Text]"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text]"
}

BankiParser: {
  shape: class

  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BaseSravni: {
  shape: class

  request_bank_list(): "dict[str, Any] | None"
  get_bank_reviews(bank_info SravniBankInfo, page_num int, page_size int): "dict[str, Any] | None"
  get_num_reviews(bank_info SravniBankInfo): int
  load_bank_list()
  parse_reviews(reviews_array list\[dict\[str, str\]\], last_date datetime, bank SravniBankInfo): "list[Text]"
  get_reviews(parsed_time datetime, bank_info SravniBankInfo): "list[Text]"
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
  parse()
  width: 1349
  height: 460
}

SravniReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): "str"
}

SravniInsuranceReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
}

SravniMfoReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
}

VKParser: {
  shape: class

  load_bank_list()
  json_to_comment_text(domain str, comment dict\[str, Any\], bank_id int, is_thread bool = False): Text
  get_post_comments(domain str, owner_id str, post_id str, comments_num int, bank_id int): "list[Text]"
  get_vk_source_params(source Source): "tuple[int, int, int, datetime]"
  parse()
  width: 1286
  height: 322
}

BaseParser -> BankiBase
BankiBase -> BankiBroker
BankiBase -> BankiInsurance
BankiBase -> MfoParser
BankiParser -> BankiNewsParser
BankiBase -> BankiParser
BaseParser -> BaseSravni

BaseSravni -> SravniInsuranceReviews
BaseSravni -> SravniMfoReviews
BaseParser -> VKParser
BaseSravni -> SravniReviews
BaseSravni -> SravniMfoReviews
#+end_src

#+CAPTION: Схема классов сборщиков данных
#+NAME: fig:parser_class_diagram
#+ATTR_LATEX: :placement [h!]
[[file:img/d2/parser_class.png]]
#+LATEX: \end{landscape}
* Листинг программы
В данном документе представлено описание структуры репозитория, в котором находится исходный код системы, описанной и разработанной в работе.

Репозиторий программы находится по ссылке: [[https://github.com/Samoed/EthicsAnalysis]].
* Footnotes
[fn:1] https://kontur.ru/expert, https://www.esphere.ru/products/spk/financial
[fn:2] https://proverki.gov.ru/portal/public-search

# Local Variables:
# org-latex-listings: t
# org-latex-default-packages-alist: nil
# End:
