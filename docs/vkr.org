#+STARTUP: latexpreview
#+TITLE: Разработка сайта для автоматического сбора, анализа и визуализации информации по этичности компаний
#+AUTHOR: Соломатин Роман Игоревич
#+LANGUAGE: RU
#+LATEX_CLASS: HSEUniversity
#+LATEX_CLASS_OPTIONS: [PI, VKR]
#+bibliography: library.bib
#+cite_export: biblatex
#+OPTIONS: toc:nil H:4 ':t
#+LATEX_HEADER_EXTRA: \Abstract{В данной работе проведен анализ этичности разных компаний.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В первой главе находится описание используемых алгоримов.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: Во второй главе представлено проектирование системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В третьей главе представлена реализация системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: В четвертой главе представлено тестирование работы системы.
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: Количество страниц - N, количество иллюстраций - N, количетсво таблиц - N.}

* Введение
:PROPERTIES:
:UNNUMBERED: t
:END:
Этичность компаний уже давно вызывает озабоченность, особенно в отношении их поведения в спорных ситуациях и предоставления услуг, ориентированных на клиента. В последние годы все большее внимание уделяется оценке этичности компаний[cite:@mure_esg_2021 ; @miralles-quiros_esg_2019; @climent_ethical_2018], особенно в банковском секторе и через призму экологических, социальных и управленческих факторов (ESG). Необходимость в таких оценках становится все более острой по мере того, как общество продолжает бороться с последствиями неправомерных действий корпораций и более широким воздействием корпоративной деятельности на общество и окружающую среду.

В настоящее время существует несколько сервисов, которые призваны оценивать этику компании, но эти оценки часто основаны на судебных делах и других официальных отчетах, а не на отзывах клиентов. Это привело к ситуации, когда отдельные лица должны проводить свои собственные исследования, чтобы определить насколько этична компания. Это часто включает в себя просмотр отзывов с различных веб-сайтов, что может занять много времени и не всегда может дать исчерпывающую или точную картину.

Для решения этой проблемы будет реализована система, которая собирала бы и анализировала отзывы потребителей с различных веб-сайтов, чтобы дать более полную и точную оценку этической практики компании. Такая система может быть разработана для автоматического сбора и анализа отзывов потребителей из различных источников, включая социальные сети и сайты отзывов. Затем собранные данные могут быть проанализированы с помощью различных методов, таких как обработка естественного языка и машинное обучение, для выявления закономерностей и тенденций, связанных с этической практикой компании. Полученный анализ может быть использован для разработки более надежной и достоверной системы оценки этичности компаний.

Объект исследования – деятельность компаний.

Предмет исследования – программные средства для оценки этичности деятельности компаний.

Цель работы – создание системы для оценки этичности компаний.

Исходя из поставленной цели, необходимо:

1. Провести анализ предметной области
2. Провести анализ системы
3. Реализовать систему
4. Провести тестирование системы

Этап анализа должен:
1. Анализ предметной области
2. Анализ существующих алгоритмов

Этап проектирования должен включать:
1. Проектирование серверной части
2. Проектирование модели для определения этичности
3. Проектирование клиентской части приложения

Этап реализации должен включать:
1. Описание сбора данных
2. Реализации модели
3. Реализации серверной части
4. Реализации клиентской части

Этап тестирования должен включать:
1. Тестирование модели
2. Тестирование серверной части
3. Тестирование клиентской части
* Анализ предметной области
** BERT
BERT [cite:@devlin2018bert] (Bidirectional Encoder Representations from Transformers) -- это нейросетевая языковая модель, которая показала высокую эффективность для задач обработки естественного языка, таких как классификация текстов, ответы на вопросы и распознавание именованных сущностей. Она основана на архитектуре трансформаторов, представленной в статье "Attention is All You Need" [cite:@NIPS2017_3f5ee243], которая использует механизмы самовнимания для обработки входных последовательностей параллельно, а не последовательно, как в традиционных архитектурах рекуррентных нейронных сетей.

В отличие от простых алогитмов машинного обучения, таких как, TF-IDF [cite:@jones1972statistical], мешка слов[cite:@doi:10.1080/00437956.1954.11659520] и word2vec [cite:@mikolov2013efficient] последовательности слов и предложений кодируются вектором (эмбеддингом) фиксированной длины. Это позволяет эффективно анализировать текст с пониманием контекста, что поможет при анализе отзывов.
Одной из основных причин превосходной производительности BERT является его способность работать с двунаправленным контекстом. В отличие от ELMO [cite:@elmo] и GPT[cite:@radford2019language], которые являются однонаправленными моделями, BERT обучен учитывать контекст как слева, так и справа от слова, что приводит к более точному представлению значения слов в предложении.

На вход модели подается предложение или пара предложений. Затем разделяется на отдельные слова (токены).  Потом в начало последовательности вставляется специальный токен =[CLS]=, обозначающий начало предложения или начало последовательности предложений. Пары предложений группируются в одну последовательность и разделяются с помощью специального токена =[SEP]=. Потом все токены превращаются в эмбеддинги [[fig:inputemebeddings]] по механизму внимания [cite:@NIPS2017_3f5ee243].

#+CAPTION: Пример ввода текста в модель
#+NAME: fig:inputemebeddings
#+ATTR_LATEX: :placement [h]
[[file:img/Input_Emebeddings.pdf]]

При обучении модель выполняет на 2 задания:
 1) Предсказание слова в предложении

    Поскольку стандартные языковые модели или смотрят текст слева направо, или справа налево [[fig:BERT_comparisons]], как ELMo [cite:@elmo] и GPT [cite:@radford2019language], они работают с контекстом хуже, чем данная модель. Так как BERT двунаправленный, у каждого слова можно посмотреть его контекст, что позволит предсказать замаскированное слово.

    #+CAPTION: Сравнение принципов работы BERT, ELMo, GPT
    #+NAME: fig:BERT_comparisons
    #+ATTR_LATEX: :placement [h]
    [[file:img/BERT_comparisons.pdf]]

    Это задание обучается следующим образом -- 15% случайных слов заменяются в каждом предложении на специальный токен =[MASK]=, а затем предсказываются, учитывая окружающий контекст. Однако иногда слова заменяются не на специальные токены, в 10% заменяются на случайный токен и еще в 10% заменяются на случайное слово. Этот процесс обучения позволяет модели изучить связи между словами в предложении и представить каждое слово в высокоразмерном векторном пространстве, называемом вкраплением. Эти вкрапления отражают смысл слов в предложении и могут быть использованы для представления предложения в целом.

 2) Предсказание следующего предложения

    Для того чтобы обучить модель, которая понимает отношения предложений, она предсказывает, идут ли предложения друг за другом. Для этого с 50% вероятностью выбирают предложения, которые находятся рядом и наоборот. Пример ввода пары предложений в модель [[fig:bert_pretraining]].

    #+CAPTION: Схемам работы BERT
    #+NAME: fig:bert_pretraining
    #+ATTR_LATEX: :width 0.6\textwidth :placement [!hbp]
    [[file:img/bert_pretrainin.png]]

 BERT можно использовать для получения оценки тональности отзывов о компаниях, для этого можно немного модифицировать. Он может быть обучен предсказывать, является ли отзыв положительным, отрицательным или нейтральным. После обучения модель можно будет использовать для прогнозирования тональности новых отзывов, обеспечивая надежный и эффективный способ оценки тональности отзывов о компаниях.
** Sentense BERT
Sentense BERT [cite:@reimers-2019-sentence-bert] -- это модификация предобученных моделей BERT, которая использует модель BERT и подает на вход 2 предложения, затем усреднят их выходы, а после с помощью функции ошибки выдаёт результат. Схема работы модели [[ref:fig:sbert]].
#+CAPTION: Схема работы SBERT
#+NAME: fig:sbert
#+ATTR_LATEX: :width 0.4\textwidth :placement [hbp]
[[file:img/sbert.png]]
Основное преимущество данной модели над классическим BERT: эмбеддинги предложений можно сравнивать друг с другом независимо и без необходимости пересчитывать их каждый раз. Например, если для поиска похожих предложений из 10000 для обычного BERT потребуется 50 миллионов вычислений различных пар предложений, и это займёт 50 часов, то Sentense BERT рассчитает эмбеддинг каждого предложения отдельно и потом их сравнит, и это займёт примерно 5 секунд.
** CLIP
CLIP (Contrastive Language–Image Pre-training)[cite:@radford2021learning] -- это нейронная сеть, обученная на множестве пар (изображение, текст) и способная изучать широкий спектр визуальных и лингвистических концепций, предсказывая текст, соответствующий заданному изображению.

Модель использует Visual Transformer (ViZ) [cite:@dosovitskiy2020image] для обучения представлениям изображений. ViZ обучен понимать и генерировать изображения, а трансформер[cite:@NIPS2017_3f5ee243] обучен понимать и генерировать текст. Сочетание этих двух архитектур позволяет модели CLIP одновременно изучать визуальные и лингвистические концепции.

Одним из ключевых преимуществ CLIP является его способность обучать эмбеддинги, которые не являются специфическими для конкретной задачи или области. Кроме того, CLIP можно точно настроить на наборе данных, специфичном для конкретной задачи, чтобы улучшить производительность на конкретных задачах. Этот метод позволяет соединить пространства двух разных источников информации. Например, эта модель может быть адаптирована для соединения предложений из разных областей.
* Проектирование системы
** Проектирование базы данных

** Проектирование архитектуры системы
*** Проектирование серверной части
*** Проектирование клиентской части

* Реализация системы
** Реализация серверной части
*** Реализация API
*** Реализация парсера banki.ru
*** Реализация парсера sravni.ru
*** Реализация модуля обработки текста
** Реализация клиентской части
* Тестирование системы
* Заключение
:PROPERTIES:
:UNNUMBERED: t
:END:
#+latex: %\nocite{*}
#+LATEX: \putbibliography
#+LATEX: \appendix
