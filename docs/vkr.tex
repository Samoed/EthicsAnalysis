\documentclass{article}

\begin{document}

\maketitle

\chapter*{Введение}
\label{sec:org4445a4e}
Эффективность работы компаний зависит от многих факторов, в том числе и от этики делового поведения и ответственности их сотрудников. Компании, которые придерживаются высоких стандартов этики и интегрируют их в свою культуру, обычно имеют более лояльных клиентов и успешнее конкурируют на рынке. Кроме того, соблюдение этических норм и принципов способствует укреплению репутации компании, что может привести к привлечению талантливых сотрудников и установлению долгосрочных партнерских отношений с другими компаниями и организациями. В целом, этика делового поведения играет важную роль в формировании имиджа компании и ее успеха на рынке.

Этика компаний – это разделяемые всеми сотрудниками организации правила и нормы, ценности и убеждения, манера общения и другие факторы, которые регламентируют поведение и взаимодействии членов компании. Существует 3 уровня этики компаний~\autocite{smirnova_biznesetika_2021}:
\begin{enumerate}
\item Мировой -- отвечает за увеличение общественного благосостояния, обеспечение рабочих мест, научно-технические инновации и модернизацию производственных процессов и так далее.
\item Макроуровень -- отвечает за принципы рыночной конкуренции, информационной прозрачность и равнодоступности для всех участников рынка и так далее.
\item Микроуровeнь -- отвечает за доверие и отсутствие дискриминации в отношениях между контрагентами, между сотрудниками и менеджерами, морально-нравственный климат в организации и так далее.
\end{enumerate}

В данной работе будет рассматриваться этика на микроуровне, как вероятность принятия компанией каких-то действий, которые краткосрочно не обязательно выигрышных для бизнеса, но которые увеличивают лояльность клиентов компании. Например, у клиента банка задержали зарплату и он не делает платеж по кредиту. Формально банк может по кредитному договору назначить штраф за неисполнение клиентом обязательств, но войдя в положение клиента, банк может не назначить или отменить такой штраф.

В настоящее время существует несколько сервисов, которые призваны оценивать этику компании на основании финансовых показателей\footnote{\url{https://kontur.ru/expert}, \url{https://www.esphere.ru/products/spk/financial}} и судебных дел\footnote{\url{https://proverki.gov.ru/portal/public-search}}. Это привело к ситуации, когда отдельные лица должны проводить свои собственные исследования, чтобы определить насколько этична компания. Это часто включает в себя просмотр отзывов с различных веб-сайтов, что может занять много времени и не всегда может дать исчерпывающую или точную картину, так как не включает в себя качество обслуживания.

Для решения этой проблемы реализована система, которая собирает и анализирует отзывы потребителей с различных веб-сайтов, чтобы дать более полную и точную оценку этической практики компании. Затем собранные данные анализируются с помощью различных методов, таких как обработка естественного языка и машинного обучения, для выявления закономерностей и тенденций, связанных с этической практикой компании. Полученный анализ может быть использован для разработки более надежной и достоверной системы оценки этичности компаний.

Объект исследования -- методы оценки этичности компаний.

Предмет исследования -- автоматизация оценки этичности компаний на основании отзывов клиентов.

Цель работы – создание системы для оценки этичности компаний.

Исходя из поставленной цели, необходимо:

\begin{enumerate}
\item Провести анализ предметной области и требований.
\item Реализовать систему.
\item Провести тестирование системы.
\end{enumerate}

Этап анализа должен:
\begin{enumerate}
\item Анализ предметной области.
\item Анализ требований к системе.
\item Анализ существующих алгоритмов.
\end{enumerate}

Этап проектирования должен включать:
\begin{enumerate}
\item Проектирование серверной части.
\item Проектирование модели для определения этичности.
\item Проектирование клиентской части приложения.
\end{enumerate}

Этап реализации должен включать:
\begin{enumerate}
\item Описание сбора данных.
\item Реализации модели.
\item Реализации серверной части.
\item Реализации клиентской части.
\end{enumerate}

Этап тестирования должен включать:
\begin{enumerate}
\item Тестирование модели.
\item Тестирование серверной части.
\item Тестирование клиентской части.
\end{enumerate}

В ходе выполнения анализа, проектирования и реализации приложения используется объектно-ориентированный подход. Результаты анализа и решения задач проектирования формализуются с помощью диаграмм \texttt{UML}. При разработке базы данных используется реляционная СУБД \texttt{PostgreSQL}, а серверная часть приложения реализуется на языке python~\autocite{vanrossum_python_2009} с помощью фреймворка \texttt{FastApi}, а алгоритмы анализы текста будут использовать методы машинного обучения.
\chapter{Анализ предметной области}
\label{sec:org8176655}
В данной главе представлен аналитический обзор оценок этичности компаний и алгоритмов машинного обучения, а также обзор существующих программных решений для поставленной проблемы.

Анализ предметной области следует разделить на следующие пункты:
\begin{enumerate}
\item Анализ процесса определения этичности компаний сейчас позволяет понять, как этот процесс сейчас происходит и как его лучше всего автоматизировать.
\item Анализ оценок этичности компаний для того, чтобы в дальнейшем определить этичность компаний.
\item Анализ существующих решений выполняется с целью выделения их сильных и слабых сторон по отношению к решаемой проблеме и обоснования необходимости разработки нового средства, подходящего под регламент задач.
\item Анализ алгоритмов позволяет понять с помощью каких алгоритмов можно найти полезную информацию в текстах.
\item Анализ требований к системе позволит выделить функциональные и нефункциональные требования.
\end{enumerate}
\section{Анализ определения этичности компании}
\label{sec:org7362914}
Этичность компаний уже давно вызывает озабоченность, особенно их поведение в спорных ситуациях и предоставление услуг, ориентированных на клиента. В последние годы все большее внимание уделяется оценке этичности компаний~\autocites{mure_esg_2021}[][]{semenko_korporativnaya_2022}[][]{kudryavceva_korporativnosocialnaya_2016}, особенно в банковском секторе и через призму экологических, социальных и управленческих факторов (ESG). Необходимость в таких оценках становится все более острой по мере того, как общество продолжает бороться с последствиями неправомерных действий корпораций и более широким воздействием корпоративной деятельности на общество и окружающую среду.

Сейчас процесс поиска этичной компании выгладит следующим образом: сначала ищутся компании, которые предоставляют желаемые услуги. Далее они изучаются, чтобы определить их этичность. Этот процесс включает в себя:
\begin{enumerate}
\item Просмотр отчетности компании.
\item Анализ ее финансовой деятельности.
\item Изучение информации о социальной ответственности.
\end{enumerate}

Для этого клиенты компаний обращаются к различным источникам информации, таким как веб-сайты компаний, рейтинговые агентства, исследовательские организации и другие источники. Потом, изучаются социальные сети компании или отзывы пользователей на разных сайтах, форумах и социальных сетях, чтобы получить дополнительную информацию и оценить общее мнение о компании. После изучения каждой компании люди выбирают ту, которую они считают наиболее этичной и социально ответственной. Блок-схема данного поиска рис.~\ref{fig:as_is}. Важным фактором для определения этичности компании может быть ее социальная ответственность, устойчивость бизнеса и соблюдение норм и стандартов в области финансовой деятельности.

В целом, процесс поиска компаний и определения их этичности может быть длительным и требует серьезного подхода. Люди могут использовать различные источники информации, чтобы сделать осознанный выбор и инвестировать свои деньги в компанию, которая соответствует их ожиданиям и требованиям.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{img/mermaid/as_is.png}
\caption{\label{fig:as_is}Диаграмма того, как сейчас происходит поиск компании}
\end{figure}

\section{Анализ оценок этичности компаний}
\label{sec:orgaddc441}
Оценка этики компании -- это не одноразовый процесс, а скорее непрерывная попытка понять и оценить действия, политику и практику компании с течением времени. Это включает в себя рассмотрение соблюдения компанией отраслевых этических стандартов и передовой практики, а также мониторинг любых изменений в этической позиции компании с течением времени. Кроме того, участие в диалоге с компанией и консультации с организациями, специализирующимися на оценке корпоративной ответственности могут дать ценную информацию об этических практиках компании.

Компаниям важно оставаться этичными, так как на долгосрочной перспективе это приносит большую прибыль и улучшает показатели бизнеса, чем неэтичный способ ведение бизнеса\autocites{climent_ethical_2018}[][]{mure_esg_2021}. Насколько этична компания можно рассматривать с двух сторон, самой компании и их клиентов. Со стороны компаний можно выделить факторы, которые можно получить из их отчетности:
\begin{itemize}
\item количество капитала, чтобы они не могли обанкротиться.
\item какое влияние они вносят на окружающую среду.
\item куда идут инвестиции~\autocite{harvey_ethical_1995}.
\end{itemize}
Для пользователей одними из ключевых факторов можно выделить:
\begin{itemize}
\item качество пользовательского сервиса~\autocite{brunk_exploring_2010}, как правило пользователи оставляют отзывы на сайтах по пяти бальной шкале.
\item насколько навязчивые услуги компании~\autocite{mitchell_bank_1992}, как правило пользователи оставляют отзывы на сайтах по пяти бальной шкале.
\end{itemize}

В данной работе этичность компаний будет определяться по отзывам клиентов, которые освещают проблемы качества услуг и качество сервиса, и на основе отчетности компаний, что позволит полностью осветить проблему. Для анализа текстов будут использоваться алгоритмы машинного обучения.
\section{Анализ существующих решений}
\label{sec:org15aa589}
Существует несколько индексов, предназначенных для измерения этичности -- индекс Dow Jones Sustainability Indices~(DJSI)~\autocite{lopez_sustainable_2007} и FTSE4GOOD~\autocite{collison_financial_2008}.

DJSI оценивает показатели устойчивости компаний различных секторов на основе экономических, экологических и социальных критериев. Компании отбираются на основе их показателей по сравнению с аналогичными компаниями в том же секторе. Процесс оценки включает в себя тщательную оценку компаний по различным критериям, включая корпоративное управление, экологический менеджмент, трудовую практику, права человека и социальные вопросы.

Аналогичным образом, индекс FTSE4GOOD предназначен для оценки деятельности компаний, которые демонстрируют эффективную практику экологического, социального и управленческого менеджмента (ESG). Компании отбираются на основе их практики ESG и оцениваются по различным критериям, включая изменение климата, права человека и корпоративное управление.

Индексы DJSI и FTSE4GOOD разработаны для того, чтобы помочь инвесторам определить компании, которые привержены этической практике. Эти индексы предоставляют инвесторам стандартизированный способ сравнения компаний на основе их показателей. Это помогает инвесторам принимать более обоснованные инвестиционные решения и побуждает компании внедрять устойчивую практику для привлечения инвестиций.

Для российских компаний нет аналогичных индексов. Сейчас данные об этичности компаний можно получить из агрегаторов отзывов и отчётности. Агрегаторы позволяют собрать информацию о клиентском обслуживании, а отчетность компаний о положении дел в целом. Но сейчас не существует способов, как можно оценить все вместе.
\section{Алгоритмы для анализа текста}
\label{sec:org517f899}
Алгоритмы машинного обучения для анализа текста получили широкое распространение для извлечения информации из неструктурированных данных с помощью больших помеченных наборов данных. Среди различных используемых методов несколько алгоритмов оказались особенно эффективными в этой области. К ним относятся мешок слов~\autocite{harris_distributional_1954}, TF-IDF~\autocite{jones_karen_sparck_statistical_1972}, Word2Vec~\autocite{mikolov_distributed_2013}, ELMO~\autocite{peters_deep_2018}, GPT~\autocite{radford_language_2019}, BERT~\autocite{devlin_bert_2019} и другие. Каждый из этих алгоритмов обладает уникальными характеристиками, которые делают их хорошо подходящими для определенных задач.

Модель {}<<Мешок слов>>{} представляет текстовые данные путем присвоения уникального номера каждому слову в документе. Этот метод прост в реализации, но не учитывает порядок слов в предложении. С другой стороны, модель TF-IDF представляет текстовые данные, учитывая как частоту слова в документе (TF), так и его редкость во всех документах корпуса (IDF). Этот подход может быть использован для определения важности слова в данном документе и обычно используется в задачах поиска информации и обработки естественного языка, но он не использует контекста употребления слов, не имея возможность различать в том числе омонимию.

Word2Vec использует векторное представление слов, что позволяет алгоритму улавливать значение слов в сходных контекстах. Это позволяет более точно представлять взаимосвязи между словами, что приводит к повышению производительности в таких задачах, как классификация текста и анализ настроений. Однако, этот метод все ещё неспособен различать смысл слова в зависимости от конеткста, так как каждое слово имеет единственную {}<<кодировку>>{}.

ELMO, GPT и BERT, с другой стороны, основаны на архитектуре трансформеров, в которой каждое предложение представлено вектором чисел, обычно известным как вложение. Такое представление позволяет получить более полное и целостное понимание текста, поскольку оно учитывает контекст всего предложения или текста.

Из этих алгоритмов BERT считается наиболее продвинутым и мощным, поскольку он способен учитывать контекст всего предложения или текста, в то время как GPT и ELMO рассматривают только односторонний контекст. Это позволяет BERT достигать самых современных результатов в широком спектре задач анализа естественного языка.

Таблица результата сравнения моделей \ref{tbl:model_compare}.
\begin{table}[h!]
\caption{\label{tbl:model_compare}Сравнение моделей}
\centering
\begin{tabular}{|c|c|c|}
\hline
Модель & Вектор слов & Контекст\\[0pt]
\hline
Мешок слов & зависит от количества слов & нет\\[0pt]
\hline
TF-IDF & зависит от количества слов & очень слабо\\[0pt]
\hline
Word2Vec & не зависит от количества слов & слабо\\[0pt]
\hline
ELMO & не зависит от количества слов & однонаправленный\\[0pt]
\hline
GPT & не зависит от количества слов & однонаправленный\\[0pt]
\hline
BERT & не зависит от количества слов & двунаправленный\\[0pt]
\hline
\end{tabular}
\end{table}

\subsection{BERT}
\label{sec:org460bff9}
BERT~\autocite{devlin_bert_2019} (Bidirectional Encoder Representations from Transformers) -- это нейросетевая языковая модель, которая относится к классу трансформеров. Она состоит из 12 «базовых блоков» (слоев), а на каждом слое 768 параметров.

На вход модели подается предложение или пара предложений. Затем разделяется на отдельные слова (токены). Потом в начало последовательности токенов вставляется специальный токен \texttt{[CLS]}, обозначающий начало предложения или начало последовательности предложений. Пары предложений группируются в одну последовательность и разделяются с помощью специального токена \texttt{[SEP]}, затем к каждому токену добавляется эмбеддинг, показывающий к какому предложению относится токен. Потом все токены превращаются в эмбеддинги~\ref{fig:inputemebeddings} по механизму описанному в работе~\autocite{vaswani_attention_2017}.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{img/Input_Emebeddings.pdf}
\caption{\label{fig:inputemebeddings}Пример ввода текста в модель}
\end{figure}

При обучении модель выполняет на 2 задания:
\begin{enumerate}
\item Предсказание слова в предложении

Поскольку стандартные языковые модели либо смотрят текст слева направо или справа налево~\ref{fig:BERT_comparisons}, как ELMo~\autocite{peters_deep_2018} и GPT~\autocite{radford_language_2019}, они не подходят под некоторые типы заданий. Так как BERT двунаправленный, у каждого слова можно посмотреть его контекст, что позволит предсказать замаскированное слово.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{img/BERT_comparisons.pdf}
\caption{\label{fig:BERT_comparisons}Сравнение принципов работы BERT, ELMo, GPT}
\end{figure}

Это задание обучается следующим образом -- 15\% случайных слов заменяются в каждом предложении на специальный токен \texttt{[MASK]}, а затем предсказываются на основании контекста. Однако иногда слова заменяются не на специальны токена, в 10\% заменяются на случайный токен и еще в 10\% заменяются на случайное слово.

\item Предсказание следующего предложения

Для того чтобы обучить модель, которая понимает отношения предложений, она предсказывает, идут ли предложения друг за другом. Для этого с 50\% вероятностью выбирают предложения, которые находятся рядом и наоборот. Пример ввода пары предложений в модель~\ref{fig:bert_pretrainin}.

\begin{figure}[hbp]
\centering
\includegraphics[width=0.6\textwidth]{img/bert_pretrainin.png}
\caption{\label{fig:bert_pretrainin}Схемам работы BERT}
\end{figure}
\end{enumerate}
\subsection{Sentence BERT}
\label{sec:orgfa18a05}
Sentense BERT~\autocite{reimers_sentence-bert_2019} -- это модификация предобученных моделей BERT, которая обрабатывает два предложения, затем усреднят их, а после с помощью функции ошибки выдаёт результат. Схема работы модели~\ref{fig:sbert}.
\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{img/sbert.png}
\caption{\label{fig:sbert}Схема работы SBERT}
\end{figure}
Основное преимущество данной модели над классическим BERT: эмбеддинги предложений можно сравнивать друг с другом независимо и не пересчитывать их пару каждый раз. Например, если для поиска похожих предложений из 10000 для обычного BERT потребуется 50 миллионов вычислений различных пар предложений, и это займёт несколько дней часов, то Sentense BERT рассчитает эмбеддинг каждого предложения отдельно, потом их сравнит. Такой способ рассчета ускоряет работу программы до нескольких секунд.
\section{Анализ требований к системе}
\label{sec:orgff14b7d}
Исходя из интервью с заказчиком система должна уметь:
\begin{enumerate}
\item Показывать историю изменений индекса с возможностью фильтровать по:
\begin{enumerate}
\item Годам.
\item Отраслям компаний, с возможностью множественного выбора.
\item Компаниям, с возможностью множественного выбора.
\item Моделям, с возможностью множественного выбора.
\item Источникам, с возможностью множественного выбора.
\end{enumerate}
\item Агрегировать значения индекса по годам и кварталам.
\item Анализировать тексты для построения индекса этичности.
\item Иметь возможность добавления анализа текста несколькими вариантами.
\item Сохранять тексты для последующего анализа другими методами.
\item Система должна собирать данные с сайтов banki.ru, sravni.ru и комментарии из групп {}<<вконтаке>>{}.
\item На сайте должен быть график, который показывать изменение индекса этичности компаний и количества собранных отзывов по разным источникам.
\item Для расчета индекса этичности компаний на основании рецензий должна использоваться формула~\ref{eq:ethics}:
\end{enumerate}

\begin{equation}
\label{eq:ethics}
\begin{aligned}
\text{Base index} &= \frac{\text{positive} - \text{negative}}{\text{positive} + \text{negative}} \\
\text{Std index} &= \sqrt{\frac{\text{positive}}{\text{negative} \cdot (\text{positive} + \text{negative})^{3}} + \frac{\text{negative}}{\text{positive} \cdot (\text{positive} + \text{negative})^{3}}} \\
\text{Index} &= ({2\cdot(({\text{Base index}}-{\text{Mean index}}) > 0) - 1})\cdot\\
            &{max\left(\left|{\text{Base index}}-{\text{Mean index}}\right|-{\text{Std index}}, 0\right)}
\end{aligned}
\end{equation}

\(positive\) -- количество позитивных предложений,

\(negative\) -- количество негативных предложений,

\(Mean\ index\) -- среднее значения для пар источник сбора данных и модели, которая обрабатывала предложения.

На основе описания функциональных требований была создана диаграмма вариантов использования, которая представлена на рисунке~\ref{fig:usecasefull}.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{img/use-case.png}
\caption{\label{fig:usecasefull}Диаграмма вариантов использования}
\end{figure}

Также были получены нефункциональные требования:
\begin{enumerate}
\item Построение графика не должно занимать больше секунды.
\item Данные должны собираться автоматическ.
\item Данные должны обрабатываться автоматически.
\item Система должны способна работать с большим объемом информации.
\item Система должна быть стабильна.
\end{enumerate}
\section{Выбор технологий для разработки}
\label{sec:org25a63f5}
Для реализации этой системы будет использоваться язык Python. Для этого языка разработано много библиотек, которые позволят быстро реализовать алгоритмы обработки естественного языка, в частности в этом проекте будет использоваться Pytorch~\autocite{paszke_pytorch_2019} и HuggingFace~\autocite{wolf_transformers_2020}, и собирать данные с сайтов. Для реализации API будет использоваться FastAPI, что позволит разработать API для системы с автоматической генерацией документации.

Хранение данных будет использоваться объектно-реляционная система управления базами данных PostgreSQL, что позволит обрабатывать большие объемы данных. Для работы с ней будет использоваться Code first подход, с помощью Python библиотек Sqlalchemy и Alembic для изменения схемы данных (миграций).
\section{Выводы по главе}
\label{sec:org48626ef}
По итогам анализа предметной области, можно сделать вывод о том, что определение этичности компаний является важной задачей, которую можно автоматизировать с помощью алгоритмов машинного обучения. Анализ оценок этичности компаний позволяет понять, какие факторы необходимо учитывать при разработке алгоритмов. Обзор существующих решений показал, что некоторые из них имеют свои преимущества и недостатки, и может потребоваться разработка нового средства, учитывающего особенности задачи. Анализ алгоритмов помогает выбрать наиболее подходящие алгоритмы для поиска полезной информации в текстах. Наконец, анализ требований к системе позволяет определить необходимые функциональные и нефункциональные требования, которые будут учитываться при разработке решения. В целом, эти аналитические пункты помогут определить оптимальный подход к решению задачи определения этичности компаний.
\chapter{Проектирование системы}
\label{sec:org9ce669b}
В данной главе представлена общая архитектура системы и каждого модуля. Описана база данных и архитектура каждого модуля отдельно.

Этап проектирования следует разделить на следующие пункты:
\begin{enumerate}
\item Определение основных компонентов приложения и проектирование архитектуры системы.
\item Проектирование базы данных и модулей.
\item Проектирование модели для обработки естественного текста.
\end{enumerate}

Данная глава предоставляет описание системы, продемонстрировать каждый компонент и их взаимосвязь в достижении желаемого результата.
\section{Проектирование архитектуры системы}
\label{sec:orgea96059}
Система будет разделена на отдельные независимые компоненты (микросервисы), что позволит ей быть надежной (если в какой-то части системы будут сбои, то остальная часть системы продолжит работать) и масштабируемой (легко добавлять новые компоненты). Каждый микросервис системы будет представлять собой docker container, которые будут управляться с помощью docker compose. Каждый сервис будет реализовывать отдельный компонент бизнес-логики и коммуницировать с другими компонентами через REST API.

Было выделено 4 главных компонента бизнес логики:
\begin{enumerate}
\item Работа с базой данных -- это HTTP API, который обеспечивает возможность сохранения и получения данных из базы данных. Данный компонент принимает запросы на сохранение данных, получение информации из базы данных и возвращает результаты обработки этих запросов.
\item Сбор данных -- компонент, который отвечает за сбор информации с нескольких источников. Для этого используется несколько независимых сборщиков данных, которые работают с различными сайтами и другими источниками.
\item Обработка данных -- данный компонент содержит несколько моделей, которые используются для анализа данных. Эти модели производят различные виды анализа, от простой фильтрации и сортировки до более сложных операций анализа и прогнозирования.
\item Агрегирование данных -- этот компонент отвечает за агрегацию обработанных данных в единый индекс. Данный индекс может быть использован для удобного представления полученных результатов в виде отчетов и графиков. Данный модуль нужен для того чтобы быстро получать новые графики, так как агрегирование всех отзывов для компаний может занимать много времени.
\end{enumerate}

Результат архитектуры системы на рис.~\ref{fig:architecture}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/architecture.png}
\caption{\label{fig:architecture}Диаграмма архитектуры системы}
\end{figure}

Сервис для работы с базой данных, который будет обеспечивать сохранение и получение информации из различных сервисов сбора и обработки данных. Для этого будет предоставлен API, который будет использоваться для отправки и получения данных.

Сервисы сбора данных будут отправлять собранные тексты в формате JSON на сервис работы с базой данных с помощью HTTP запросов. Кроме того, информация, необходимая для сбора данных, будет храниться в базах данных соответствующих сервисов.

Сервис агрегации данных будет периодически обновлять базу данных один раз в день для обеспечения актуальности данных.

Сервис сбора данных будет включать несколько моделей машинного обучения, которые будут использоваться для анализа данных, полученных из сервиса сбора данных. После обработки данных, результаты будут отправляться обратно в сервис сбора данных.
\section{Проектирование базы данных}
\label{sec:org1d40d58}
Исходя из поставленных требований было решено разделить базу данных на 2 подчасти:
\begin{enumerate}
\item Основная база данных будет хранить данные.
\item База данных для агрегации будет позволять быстро получать агрегированные данные.
\end{enumerate}

\subsection{Проектирование основной базы данных}
\label{sec:orgce27154}
На основании требований была разработана следующая схема базы данных:

Таблица сфер компаний позволяет в дальнейшей удобно фильтровать данные в зависимости от типа компании.

\begin{center}
\begin{longtblr}[caption={Таблица сфера компании\label{tbl:company_type}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Сфера компании & Строка & \\[0pt]
\end{longtblr}
\end{center}

\begin{center}
\begin{longtblr}[caption={Таблица компаний\label{tbl:companies}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Название компании & Строка & \\[0pt]
Описание компании & Строка & Дополнительное поле для сохранения вспомогательной информации о компании\\[0pt]
Лицензия компании & Строка & По лицензии компаний может будет сопоставлять компании на разных сайтах\\[0pt]
Код сферы компании & Целое & Внешний ключ из таблицы Сфера компании\\[0pt]
\end{longtblr}
\end{center}

Аналогично для сфер компаний таблица для типов источников позволяет удобно работать с данными в дальнейшем.

\begin{center}
\begin{longtblr}[caption={Таблица тип источников\label{tbl:source_type}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Название типа источника & Строка & \\[0pt]
\end{longtblr}
\end{center}

Таблица источников будет хранить информацию об источниках и когда было последнее обновление данных для них (в полях {}<<состояние сборщика данных>>{} и {}<<дата последнего сбора данных>>{}). Поле {}<<состояние сборщика данных>>{} будет иметь формат json, так как для разных источников информации потребуется сохранять информацию в различном виде и сложно определить наиболее подходящий формат заранее.

\begin{center}
\begin{longtblr}[caption={Таблица источники\label{tbl:sources}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Сайт & Строка & Сайт источника\\[0pt]
Код типа источника & Целое & Внешний ключ из таблицы тип источника\\[0pt]
Состояние сборщика данных & JSON & Данные о текущем состояние сборщика данных, если возникнет сбой\\[0pt]
Дата последнего сбора & DateTime & Точка когда сбор данных закончился, для дальнейшего сбора данных\\[0pt]
\end{longtblr}
\end{center}

Аналогично для сфер компаний таблица для типов модели позволяет удобно работать с данными в дальнейшем.

\begin{center}
\begin{longtblr}[caption={Таблица тип модели\label{tbl:model_type}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Название модели & Строка & \\[0pt]
\end{longtblr}
\end{center}

\begin{center}
\begin{longtblr}[caption={Таблица модели\label{tbl:model}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Название модели & Строка & \\[0pt]
Код типа модели & Целое & Внешний ключ на таблицу тип модели\\[0pt]
\end{longtblr}
\end{center}

\begin{center}
\begin{longtblr}[caption={Таблицы текст\label{tbl:text}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Ссылка & Строка & Ссылка на текст\\[0pt]
Код источника & Целое & Внешний ключ из таблицы источники\\[0pt]
Дата текста & DateTime & Время публикации текста\\[0pt]
Заголовок & Строка & Заголовок текста\\[0pt]
Код компании & Целое & Внешний ключ на компанию\\[0pt]
Количество комментариев & Целое & \\[0pt]
\end{longtblr}
\end{center}

Так как Bert на вход принимает отдельные предложения, было решено сделать для них отдельную таблицу.

\begin{center}
\begin{longtblr}[caption={Таблица предложений\label{tbl:sentence}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Код текста & Целое & Внешний ключ из таблицы тексты\\[0pt]
Предложение & Строка & \\[0pt]
Номер предложения & Целое & Порядковый номер предложения в тексте\\[0pt]
\end{longtblr}
\end{center}

Так как результат работы модели может отличать в зависимости от ее типа, то поле {}<<результат>>{} будет массивом.

\begin{center}
\begin{longtblr}[caption={Таблица результатов анализа текстов\label{tbl:text_result}}]{colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Назначение}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Код предложения & Целое & Внешний ключ из таблицы предложения\\[0pt]
Код модели & Целое & Внешний ключ из таблицы модели\\[0pt]
Результат & Вещественный массив & Результат работы модели\\[0pt]
Обработано & Логическое & Показатель, обработано ли предложение или нет\\[0pt]
\end{longtblr}
\end{center}

Диаграмма полученной схемы базы данных рис.~\ref{fig:database}.
\subsection{Проектирование базы данных для агрегации}
\label{sec:org9b37ab2}
При сборе функциональных требований было выявлено, что надо быстро показывать количество собранных отзывов и индекс компаний.

Обработанные данные из таблицы~\ref{tbl:text_result} агрегируются для каждого квартала и рассчитываются по формуле \ref{eq:ethics}.
\begin{center}
\begin{longtblr}[caption={Таблица для расчета и показа индекса\label{tbl:index_calc}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Год & Целое & Год за который был агрегирован индекс\\[0pt]
Квартал & Целое & Квартал за который был агрегирован индекс\\[0pt]
Название модели & Строка & \\[0pt]
Сайт источника & Строка & \\[0pt]
Тип источника & Строка & \\[0pt]
Название банка & Строка & \\[0pt]
Код банка & Целое & Для запросов через API\\[0pt]
Нейтральный & Целое & Количество нейтральных предложений за период\\[0pt]
Позитивный & Целое & Количество позитивных предложений за период\\[0pt]
Негативный & Целое & Количество негативных предложений за период\\[0pt]
Базовый индекс & Вещественное & Индекс для расчета итогового индекса\\[0pt]
Средний индекс & Вещественное & Индекс для расчета итогового индекса\\[0pt]
Std индекс & Вещественное & Индекс для расчета итогового индекса\\[0pt]
Индекс & Вещественное & Рассчитанный индекс\\[0pt]
\end{longtblr}
\end{center}

Собранные отзывы из таблицы~\ref{tbl:text} агрегируются для каждого месяца и рассчитывается количество собранных отзывов за месяц.
\begin{center}
\begin{longtblr}[caption={Таблица для расчета и показа индекса\label{tbl:index_calc}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Дата & DateTime & \\[0pt]
Квартал & Целое & Квартал за который был агрегирован индекс\\[0pt]
Тип источника & Строка & \\[0pt]
Сайт & Строка & \\[0pt]
Количество отзывов & Целое & \\[0pt]
\end{longtblr}
\end{center}

Диаграмма полученной схемы базы данных рис.~\ref{fig:database_views}.
\section{Проектирование модуля работы с данными}
\label{sec:org422fb35}
Модуль будет представлять собой HTTP API для работой с базой данных.

Для работы с базой данных будут созданы классы, которые представляют ORM-модель для работы с базой данных.

При первом старте приложение будет получаться список компаний (банки, брокеры, микрокредитные организации и страховые) с сайта {}<<Центрального банка России>>{} и помещаться в базу данных. Из этих данных будет собираться лицензия компании и название компании, для микрокредитных организаций дополнительно будет собираться основной государственный регистрационный номер (ОГРН), так как под одной лицензией может работать несколько компаний. При последующих стартах приложение будет проверяться, что в каждом списке есть компании и новые компании не будут выгружаться.

Далее создаются объекты класса Bank с использованием полученных данных и добавляются в список  cbr\_banks, затем он возвращается как результат работы функции.

Таким образом, принцип работы данного алгоритма заключается в извлечении необходимых данных из HTML-кода веб-страницы и преобразовании их в объекты класса Bank, что позволяет автоматизировать процесс получения и анализа информации о банках. Диаграмма классов рис.~\ref{fig:cbr_parser_class}.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{img/d2/cbr_parser_class.png}
\caption{\label{fig:cbr_parser_class}Диаграмма классов для сбора данных с сайта ЦБ}
\end{figure}

Для работы с источниками текстов необходимо сделать запросы для типов источников и самих источников. Также для обновления состояния сборщика данных надо сделать отдельный метод \texttt{PATCH}, который позволит обновлять время и состояние источника данных по идентификатору. Также при создании источника будет проверяться существует ли такой тип источника или нет. Если его не существует, то такой тип будет создаваться.

Сохранение текстов будет доступно по методу \texttt{POST} c передачей данных о тексте и состоянии сборщика данных. При выполнении запроса должно обновляться состояние сборщика данных, а каждый текст должен сохраняться, как набор предложений. При получении предложений должны выбираться такие предложения, которые еще не обработаны моделью.

Работа с моделями будет происходить аналогично источникам. При сохранении модели будет проверяться есть ли такой тип модели или нет. Если его нет, то он будет создан.

Также необходима возможность получения списка компаний с помощью API по различным сферам работы.

В результате проектирования должно получиться API, которое реализует запросы представленные в таблице ~\ref{tbl:api_doc}.

\include{api_table}
\section{Проектирование модуля агрегации данных}
\label{sec:org2bcaf76}
Для построения индекса этичности компаний будет ежедневно агрегироваться база данных и перестраиваться индексы.
\section{Проектирование модуля сбора данных}
\label{sec:org1e40aa8}
У всех сборщиков данных одинаковый принцип работы (рис.~\ref{fig:parser_flow}):
\begin{enumerate}
\item Сборщик данных запрашивает у модуля работы с базой данных список сохраненных компаний. Модуль отвечает на запрос, отправляя список сохраненных компаний обратно.
\item Сборщик данных запрашивает у сайта для сбора данных список компаний на сайте. Сайт отправляет список компаний обратно в сборщик данных.
\item После получения списка компаний, сборщик данных сохраняет только те компании, которые уже есть в основной базе данных. Это делается для того, чтобы связать компании которые представлены на сайте и в базе данных.
\item Затем, сборщик данных начинает собирать данные для каждой компании из списка. Это может быть сделано путем отправки запросов к API сайта или сканирования страниц сайта для поиска нужных данных. Собранные данные затем сохраняются в основной базе данных. Сбор данных будет происходить до тех пор пока не соберутся все отзывы для компании, или дата отзыва дойдет до даты предыдущего сбора данных.
\end{enumerate}

Для реализации сборщиков данных было решено сделать базовый класс, который представляет собой интерфейс с функцией \texttt{parse}. Из него наследуются интерфейсы для сбора данных для каждого сайта (banki.ru, sravni.ru, vk.com). Диаграмма классов рис.~\ref{fig:parser_class_diagram}. От этих базовых классов для каждого сайта будут наследоваться классы, которые собирают отзывы компаний из различных сфер. Было выбрано такое решение, так как представление информации в рамках одного сайта в различных разделах может сильно различаться. Также у каждого сборщика данных будет своя база данных для сохранения информации о компаниях.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/mermaid/parser_flow.png}
\caption{\label{fig:parser_flow}Схема работы сборщиков данных}
\end{figure}

\subsection{Проектирование сбора данных с banki.ru}
\label{sec:org53a967b}
Для получения данных с сайта banki.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных. Исходя из требований получилась база данных ~\ref{tbl:banki_ru}. Диаграмма полученной схемы базы данных рис.~\ref{fig:database_banki_ru}.

\begin{center}
\begin{longtblr}[caption={Таблица для сайта banki.ru\label{tbl:banki_ru}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Идентификатор банка & Целое & Идентификатор банка в основной базе данных\\[0pt]
Имя банка & Строка & \\[0pt]
Код банка & Строка & Код банка для запросов по API\\[0pt]
\end{longtblr}
\end{center}

С этого сайта будут собираться данные о компаниях из пяти сфер:
\begin{enumerate}
\item \textbf{Отзывы на банки.}
Список банков будет получаться из \url{https://www.banki.ru/widget/ajax/bank\_list.json}. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на \url{https://www.banki.ru/services/responses/list/ajax/} и в параметры ссылки будет передаваться код банка и номер страницы с отзывами и из полученного json будут собираться данные об отзывах.
\item \textbf{Новости о банках.}
В качестве списка компаний будет использоваться такой же список, как и для банков. Для получения текста новостей сначала будет собираться список новостей для компании. Для этого будут отправляться запросы на \url{https://www.banki.ru/banks/bank/\{bank.bank\_code\}/news/} в зависимости от банка. Затем по каждой ссылке будет обрабатываться html код страницы и собираться текст новости.
\item \textbf{Отзывы на страховые компании.}
Список компаний будет получаться из \url{https://www.banki.ru/insurance/companies/}. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. После этого будут собираться отзывы по \url{https://www.banki.ru/insurance/companies/}. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
\item \textbf{Отзывы на брокеров.}
Для получения списка компаний данные будут браться из \url{https://www.banki.ru/investment/brokers/list/}. Затем они будут сравниваться по номеру лицензии с брокерами, которые есть в базе данных. После этого будут собираться отзывы по \url{https://www.banki.ru/investment/responses/company/broker/}. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
\item \textbf{Отзывы на микрокредитные организации.}
Для получения списка компаний данные будут браться из \url{https://www.banki.ru/microloans/ajax/search}. Затем они будут сравниваться по номеру лицензии и ОГРН с компания, которые есть в базе данных. После этого будут собираться отзывы по \url{https://www.banki.ru/microloans/responses/ajax/responses/}. Затем из полученного json собираются отзывы о компании.
\end{enumerate}
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
\subsection{Проектирование сбора данных с sravni.ru}
\label{sec:org079c195}
Для получения данных с сайта sravni.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных, также для некоторых запросов надо иметь псевдоним компании (alias). Исходя из требований получилась база данных~\ref{tbl:sravni_ru}. Диаграмма полученной схемы базы данных рис.~\ref{fig:database_sravni_ru}.

\begin{center}
\begin{longtblr}[caption={Таблица для сайта sravni.ru\label{tbl:sravni_ru}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Идентификатор банка & Целое & Идентификатор банка в основной базе данных\\[0pt]
Код банка в sravni.ru & Целое & \\[0pt]
Старый код банка в sravni.ru & Целое & \\[0pt]
Псевдоним компании & Строка & \\[0pt]
Название банка & Строка & \\[0pt]
\end{longtblr}
\end{center}
Диаграмма полученной схемы базы данных рис.~\ref{fig:database_sravni_ru}

С этого сайта будут собираться данные о компаниях из трех сфер:
\begin{enumerate}
\item \textbf{Отзывы на банки.}
Список банков будет получаться из \url{https://www.sravni.ru/proxy-organizations/organizations} с параметром \texttt{organizationType} равным \texttt{bank}. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на \url{https://www.sravni.ru/bank/\{bank\_info.alias\}/otzyvy/} и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
\item \textbf{Отзывы на страховые компании.}
Список банков будет получаться из \url{https://www.sravni.ru/proxy-organizations/organizations} с параметром \texttt{organizationType} равным \texttt{insuranceCompany}. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на \url{https://www.sravni.ru/strakhovaja-kompanija/\{bank\_info.alias\}/otzyvy/} и в параметры ссылки будет передаваться псевдоним страховой и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
\item \textbf{Отзывы на микрокредитные организации.}
Список банков будет получаться из \url{https://www.sravni.ru/proxy-organizations/organizations} с параметром \texttt{organizationType} равным \texttt{mfo}. Затем они будут сравниваться по номеру лицензии и ОГРН с компаниями, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на \url{https://www.sravni.ru/zaimy/\{bank\_info.alias\}/otzyvy/} и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
\end{enumerate}
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
\subsection{Проектирование сбора данных с vk.com}
\label{sec:org80183a8}
Для получения на сайт vk.com будут отправляться запросы на их API. Для этого предварительно будут собраны данные о всех организациях, которые у них представлены на сайте и перемещены в базу данных \ref{tbl:vk_com}. Диаграмма полученной схемы базы данных рис.~\ref{fig:database_vk_com}.
\begin{center}
\begin{longtblr}[caption={Таблица для сайта vk.com\label{tbl:vk_com}}]{colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines}
\textbf{Название} & \textbf{Тип} & \textbf{Описание}\\[0pt]
Идентификатор & Целое & Уникальный идентификатор\\[0pt]
Идентификатор на vk.com & Строка & \\[0pt]
Имя компании & Строка & \\[0pt]
Домен компании на vk.com & Строка & \\[0pt]
\end{longtblr}
\end{center}

Для доступа к API будет зарегистрировано приложение для получения ключа к нему. Для каждой компании будут выгружаться посты пока дата последней выгрузки не более чем дата последнего поста для этого будет отправляться запрос на \url{https://api.vk.com/method/wall.get}, куда будет подставляться токен приложения и идентификатор группы. Затем для каждого поста будут выгружаться комментарии по методу \url{https://api.vk.com/method/wall.getComments}, а затем отправляться в модуль работы с базой данных.
\section{Проектирование модуля обработки данных}
\label{sec:org41d757a}
Модуль обработки данных будет представлять собой дообученную нейронную сеть Sentence-BERT. В качестве основы для обучения будет RuBERT~\autocite{kuratov_adaptation_2019}. При использовании BERT для задачи классификации, добавляется небольшой слой нейронной сети в конце предобученной модели, который выполняет финальную классификацию. Этот слой называется {}<<головой классификации>>{} (classification head). Голова классификации содержит несколько слоев нейронной сети, которые принимают входные векторы, выходные значения которых интерпретируются как вероятности принадлежности к различным классам. Количество выходных нейронов в голове классификации равно количеству классов в вашей задаче. При дообучении BERT для задачи классификации, веса всех слоев в предобученной модели остаются неизменными, а только голова классификации обучается на задаче классификации с использованием обучающей выборки. Таким образом, голова классификации добавляется к BERT при задаче классификации, и обучается на конкретной задаче классификации, используя представления слов, полученные от предобученной модели BERT.

В этой работе для дообучения будет использоваться набор из 20,000 предложений, размеченный экспертами на соответствие этическим практикам. Так как классы предложений получились не сбалансированными~\ref{fig:class_balance} (отрицательных предложений больше, на диаграмме обозначены как {}<<->>{}, чем положительных {}<<+>>{}), то в качестве основной метрики будет использоваться F1.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/class_balance.png}
\caption{\label{fig:class_balance}Распределение классов}
\end{figure}
\section{Выводы по главе}
\label{sec:orgbdf918e}
В данной главе были представлены результаты проектирования системы и ее отдельных компонентов и их взаимодействие, включая базы данных и API микросервисов, согласно выявленным требованиям из первой главы. Каждый микросервис был разработан с учетом принципов микросервисной архитектуры и обеспечивает определенную функциональность, необходимую для реализации системы в целом.

Была спроектирована база данных для хранения информации об отзывах, источниках, моделях и компаниях. Базы данных были спроектированы с учетом требований к масштабируемости и производительности системы.

Также были спроектированы сервисы для работы с базой данных, ее агрегацией, сбором данных и обработки данных.

Эти результаты будут использоваться при разработке и реализации системы в следующих этапах проекта.
\chapter{Реализация системы}
\label{sec:org9abfae0}
В данной главе описывается реализация системы и каждого микросервиса, обучение модели.

Этап реализации можно разделить на пункты:
\begin{enumerate}
\item Реализация базы данных.
\item Реализации модулей для собора, работы и агрегации данных.
\item Обучение модели и реализация модуля обработки данных.
\item Развертывание системы.
\end{enumerate}
\section{Реализация базы данных}
\label{sec:org34f9707}
Для хранения информации в системе была выбрана СУБД PostgreSQL. Для создания базы данных был выбран подход {}<<code first>>{}, который позволяет определить структуру базы данных в виде классов на языке Python. Для этого использовалась библиотека Sqlalchemy~\autocite{bayermichael_architecture_2012}, которая обеспечивает ORM-модель для работы с базами данных. При запуске приложения база данных будет создаваться автоматически на основе определенных классов.

Для определения структуры базы данных был создан базовый класс \texttt{DeclarativeBase}, который является родительским для всех классов, определяющих таблицы базы данных. Каждая таблица базы данных определяется в виде отдельного класса, который наследует базовый класс и содержит определения столбцов и связей между таблицами.

Для обеспечения возможности модернизации базы данных в дальнейшем была использована библиотека alembic, которая обеспечивает миграции базы данных и позволяет вносить изменения в структуру базы данных без потери данных.
\section{Реализация модуля работы с базой данных}
\label{sec:org5447f38}
Для реализации API используется асинхронный фреймворк FastAPI и для взаимодействии с базой данных асинхронная библиотека asyncpg. Для валидации приходящих данных и ответов для каждого запроса была создана своя модель с помощью библиотеки Pydantic. Также с помощью Pydantic был сделан класс для получения строки подключения к базе данных из переменных окружения.

При старте приложения сначала проверятся подключение с базой данных и проверяется ее версия, если она не актуальна, то выполняются миграции для ее актуализации. Затем проверятся список компаний, если список компаний пустой, то собирается данные о банках, брокера, страховых и микрофинансовых организациях.

Информация о банках будет собираться по ссылке \url{https://www.cbr.ru/banking\_sector/credit/FullCoList/}. Алгоритм начинается с получения объекта BeautifulSoup~\autocite{richardsonleonard_beautiful_2007}, который содержит HTML-код веб-страницы. Затем происходит итерация по всем элементам таблицы, начиная со второй строки, так как в первой находится заголовки для каждой колонки. Для каждой строки таблицы находятся все ячейки, извлекаются регистрационный номер (номер лицензии) и название банка. В списке также есть платежные небанковские кредитные организации, которые имеют буквы на конце лицензии, например \texttt{3511-К} у {}<<Деньги.Мэйл.Ру>>{}. Для этого такие номера будут разделяться по {}<<->>{} и браться номер и преобразовываться в число. Затем собранные данные помещаются в базу данных.

Для сбора данных о брокерах будет обрабатываться excel файл, который доступен по ссылке \url{https://www.cbr.ru/vfs/finmarkets/files/supervision/list\_brokers.xlsx}, с помощью библиотеки pandas~\autocite{team_pandasdev_2023}. При запуске происходит загрузка таблицы с данными о брокерах в формате Excel, после чего данные из таблицы считываются. Затем происходит итерация по строкам таблицы и для каждой строки создается экземпляр класса Bank, который содержит информацию о банке-брокере, такую как номер лицензии, наименование организации и тип банка. Для удобства хранения номера лицензии, из них удалялись все знаки {}<<->>{}.

Для сбора данных о страховых будет обрабатываться excel файл, который доступен по ссылке \url{https://www.cbr.ru/vfs/finmarkets/files/supervision/list\_ssd.xlsx}. Так как в файле много строк, которые не содержат номеров или наименований банков, то они удаляются из него. Номера лицензий хранятся в формате \texttt{СИ № 3847} или \texttt{ОС № 1083 - 05} и для получения номера берется первое число которое встретилось в строке с помощью регулярного выражения. Затем полученная информация помещается в базе данных.

Для сбора данных о микрофинансовых организациях будет обрабатываться excel файл, который доступен по ссылке \url{https://www.cbr.ru/vfs/finmarkets/files/supervision/list\_ssd.xlsx}. В этом файле номер лицензии разбит по 5 ячейкам и в части из отсутствуют числа. Поэтому отсутствующие ячейки заполняются нулями и содержание ячеек объединяется для получения результата. Потом также берется название компании и эта информация помещается в базу данных.

API было реализовано согласно требованиям описанными во второй главе.

Алгоритм получения предложений для обработки проверяет, какие из них уже были обработаны моделью, а какие - нет. Если для каждого запроса искать пересечение множества предложений, которые еще не обработаны моделью и уже обработаны, это может занять много времени. Поэтому сначала выполняется запрос~(\ref{lst:insert_unused}), который ищет предложения, еще не обработанные моделью. Если таких нет, то в таблицу с результатами добавляются 100 000 предложений с пустыми результатами, чтобы было проще искать предложения при дальнейших запросах. Затем с помощью запроса~(\ref{lst:select_unused}) из таблицы с результатами выбираются предложения, еще не обработанные моделью. Ниже приведены SQL запросы, которые генерирует ORM.

\begin{lstlisting}[language=SQL,label=lst:insert_unused,caption={SQL запрос на вставку не обработанных предложений},captionpos=b,numbers=none]
INSERT INTO text_result (text_sentence_id, model_id, is_processed)
SELECT text_sentence.id, :model_id, false
FROM text_sentence
JOIN text ON text_result.text_id = text.id
JOIN source ON text.source_id = source.id
LEFT JOIN (
  SELECT text_result.text_sentence_id
  FROM text_result
  WHERE text_result.model_id = :model_id
) AS subq ON text_sentence.id = subq.text_sentence_id
WHERE source.site IN (:sources) AND subq.text_sentence_id IS NULL
LIMIT 100000;
\end{lstlisting}

\begin{lstlisting}[language=SQL,label=lst:select_unused,caption={SQL запрос на получение еще не обработанных предложений},captionpos=b,numbers=none]
SELECT text_sentence.id, text_sentence.sentence
FROM text_sentence
JOIN (
  SELECT text_result.text_sentence_id, text_result.id
  FROM text_result
  WHERE text_result.model_id = :model_id AND text_result.is_processed = false
  LIMIT :limit
) AS sub
ON text_sentence.id = sub.text_sentence_id;
\end{lstlisting}

Для разделение текста на предложения при получении текста используется библиотека \texttt{nltk}~\autocite{bird_natural_2009}.

Для валидации параметров отвечающих за тип индекса этичности, список источников и период агрегации для получения агрегированных данных были сделаны \texttt{Enum}-классы. Если в запрос для получения статистики был передан параметр показывающий, что надо агрегировать только по годам, то в запрос подставлялась дополнительная часть с \texttt{group by}.

Для получения данных об обработанных предложения в зависимости от типа запрашиваемого индекса в запрос подставлялся нужный тип индекса и проводилась агрегация данных аналогично запросу на получение статистики.
\section{Реализация модуля агрегации данных}
\label{sec:orgfb2138a}
Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции.

При запуске модуля начинается подсчет количества собранных отзывов и расчет индекса этичности в разных потоках.

Так как в базе данных находится очень много элементов, то было решено обновлять данные напрямую из SQL. Код запроса на расчет статистки~\ref{lst:count_reviews}.

\begin{lstlisting}[language=SQL,label=lst:count_reviews,caption={SQL запрос на подсчет количества предложений},captionpos=b,numbers=none]
INSERT INTO text_sentence_count (count_reviews, date, quarter, source_type, source_site)
SELECT COUNT(text.id) AS reviews_count,
       DATE_TRUNC('month', text.date) AS month,
       EXTRACT('quarter' FROM text.date) AS quarter,
       source_type.name AS source_type,
       source.site AS source_site
FROM text
JOIN source ON text.source_id = source.id
JOIN source_type ON source.source_type_id = source_type.id
GROUP BY month, quarter, source.site, source_type.name;
\end{lstlisting}

Запрос для создания запроса~\ref{lst:sql_aggregate} на расчет данных было решено использовать несколько подзапросов:
\begin{enumerate}
\item Сначала рассчитывается логарифм результата обработки предложений для каждой колонки. Для избежания проблем с логарифмами к каждому значению добавляется маленькое число, так как у некоторые значения могут быть нулевыми. Этот подзапрос создан для того, чтобы ускорить выполнение, так как этот расчет можно было объединить со следующим подзапросом, но из-за этого пришлось бы пересчитывать одинаковые значения несколько раз.
\item Затем для подсчета предложений разных типов определяется их категория. Для этого используется конструкция \texttt{case when}, где значение обработанных категорий сравнивается попарно.
\item Потом к полученным данным присоединяются данные из других таблиц. Извлекается информация о квартале и дате, значения с предыдущего шага суммируются. Сам запрос объединяется для каждого квартала компаний, для каждого источника отдельно.
\item И в конце полученные данные вставляются в таблицу.
\item Затем уже на агрегированных данных рассчитываются значение индекса согласно формуле~\ref{eq:ethics}.
\end{enumerate}

\begin{lstlisting}[language=SQL,label=lst:sql_aggregate,caption={SQL запрос на агрегацию обработанных предложений},captionpos=b,numbers=none]
INSERT INTO aggregate_table_model_result (bank_id, bank_name, quater, year, model_name, source_site, source_type, positive, neutral, negative, total)
SELECT
    extract(year from text.date) as year,
    extract(QUARTER from text.date) as quarter,
    bank.id as "bank_id",
    model.name as "model_name",
    source.site as "source_site",
    source_type.name as "source_type_name",
    sum(positive) as "positive",
    sum(neutral) as "neutral",
    sum(negative) as "negative",
    sum(positive+neutral+negative) as total
FROM
    (SELECT
        text_sentence_id,
        model_id,
        case when (log_positive > log_neutral) and (log_positive > log_negative) then 1 else 0 end as "positive",
        case when (log_neutral  > log_positive) and (log_neutral > log_negative) then 1 else 0 end as "neutral",
        case when (log_negative > log_neutral) and (log_negative > log_positive) then 1 else 0 end as "negative"
    FROM (
        SELECT
            text_sentence_id,
            model_id,
            (LOG(result[1]+0.0000001)) as "log_neutral",
            (LOG(result[2]+0.0000001)) as "log_positive",
            (LOG(result[3]+0.0000001)) as "log_negative"
        FROM text_result
        WHERE model_id = 1) t) pos_neut_neg
JOIN
    text_sentence ON pos_neut_neg.text_sentence_id = text_sentence.id
JOIN
    text ON text_sentence.text_id = text.id
JOIN
    bank ON text.bank_id = bank.id
JOIN
    source ON source.id = text.source_id
JOIN
    source_type ON source.source_type_id = source_type.id
JOIN
    model ON model.id = pos_neut_neg.model_ida
GROUP BY quarter, year, source.site, source_type.name, bank.id, model.name
\end{lstlisting}
\section{Реализация модуля сбора данных}
\label{sec:org8d27298}

Для каждого сайта будет создана отдельная папка (модуль) со схожей структурой:

Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции, для обработки html страниц используется библиотека BeautifulSoup, также для обработки данных используется библиотека Pydantic.
\begin{enumerate}
\item В файле \texttt{database} будет лежать схема модели базы данных.
\item \texttt{schemes} pydantic модели для обработки текста.
\item \texttt{queries} запросы в базу данных.
\end{enumerate}

Также для всех сборщиков данных была выделена общая часть, включающая модуль запросов, модулей объектов и настроек, а также модуль для запросов к базе данных. Модуль запросов является модификацией библиотеки requests~\autocite{chandra_python_2015} и предоставляет возможность повторного выполнения запросов в случае неудачи и обработки формата json. Модуль моделей содержит pydantic классы объектов для работы с запросами к базе данных и обработки данных. Модуль настроек представляет pydantic класс, который получает данные о подключении к базе данных, ссылке на API и токен для работы с API ВКонтаке из окружения приложения. Модуль для запросов к API предоставляет набор функций для выполнения запросов.

Для удобства развертывания было решено запускать сборщик данных в зависимости от аргумента с которым запущен код. Потом при запуске в зависимости от переданных аргументов создается база данных и запускается сборщик. Процесс сбора данных запускается ежедневно с помощью библиотеки schedule.

\subsection{Разработка модуля сбора данных с banki.ru}
\label{sec:orgff8f83e}
Сбор данных с bani.ru осложнен тем, что компании из разных сфер имеют разное представление на сайте, поэтому для каждой сферы нужен свой подход. Также стоит отметить, что для успешной отправки запросов на сайт, требуется в заголовках запроса добавлять параметр {}<<X-Requested-With>>{} со значением {}<<XMLHttpRequest>>{}.

Для сбора данных был создан базовый класс, который реализует главный цикл сбора дынных. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний с сайта и проверяется какие компании уже есть в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика. Для этого каждый класс должен будет реализовать функцию для получения списка компаний \texttt{load\_bank\_list}. Затем запускается сбор данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Далее берется количество страниц отзывов у компании. Потом для каждой компании берутся тексты с помощью функции \texttt{get\_page\_bank\_reviews} и сохраняются тексты, которые не были еще собраны. Затем полученные тексты в модуль работы с базой данных.

Как реализована функция \texttt{load\_bank\_list} для различных сфер:
\begin{enumerate}
\item \textbf{Банки}. Для получения этого списка компаний будет отправляться запрос по адресу \url{https://www.banki.ru/widget/ajax/bank\_list.json} и из полученного json собираться список компаний.
\item \textbf{Страховые}. Для получения списка компаний сначала загружается html страница со списком по адресу \url{https://www.banki.ru/insurance/companies/}. Затем в ищется элемент div с атрибутом \texttt{data-module} равным \texttt{ui.pagination}. Из этого элемента из атрибута \texttt{data-options} получается количество компаний и страниц с ними. Потом для каждой страницы с компаниями ищутся все элементы \texttt{tr} с атрибутом \texttt{data-test} равным \texttt{list-row}. Из этого элемента получается вся информация о компании. Потом полученные компании сравниваются с теми, что сохранены в основной базе данных и сохраняются в базу сборщика данных.
\item \textbf{Брокеры}. Для получения списка компаний отправляется запрос по адресу \url{https://www.banki.ru/investment/brokers/list/}, но без дополнительного заголовка, так как только без него появляются лицензии компаний. Потом из этого списка собирается информация о компаниях и сохраняется в базе данных.
\item \textbf{Микрофинансовые организации}. Для получения списка компаний отправляется запрос по адресу \url{https://www.banki.ru/microloans/ajax/search}. Сначала из полученного json получается количество страниц с компаниями. Затем для каждой страницы отправляются новые запросы и обрабатывается информация о компаниях. Потом собранные компания сравниваются с компаниями из основной базы данных по номеру лицензии и ОГРН с компаниями из основной базы данных и сохраняются в базе данных сборщика данных.
\end{enumerate}

Реализация функции \texttt{get\_page\_bank\_reviews} для различных сфер:
\begin{enumerate}
\item \textbf{Банки}. Для получения отзывов будет делаться запрос по адресу \url{https://www.banki.ru/services/responses/list/ajax/} с параметрами для определения компании и номера страницы. Из полученного json соберутся отзывы и отправятся в основную базу данных.
\item \textbf{Новости}. Для получения текстов новостей сначала будут собираться адреса новостей, а затем уже сами тексты новостей. Для сбора адресов будет отправляться запрос на \url{https://www.banki.ru/banks/bank/\{bank\_code\}/news/}, где {}<<\texttt{bank\_code}>>{} код банка, также в качестве параметра запроса будет отправляться номер страницы. Для получения адресов будут браться элементы {}<<a>>{} с классом {}<<text-list-link>>{}, также для отбора новых новостей будут обрабатываться даты. Для этого будут браться элементы {}<<span>>{} с классом {}<<text-list-date>>{}. Потом по полученным ссылкам будет браться html код страниц и браться текст новости из элементов {}<<p>>{}.
\item \textbf{Страховые и Брокеры}. Для этих сфер тексты отзывов получаются путем обработки html страниц. Они получаются из запросов на адреса \url{https://www.banki.ru/investment/responses/company/broker/} для брокеров и \url{https://www.banki.ru/insurance/responses/company/} для страховых, к этим ссылкам добавляется код компании и номер страницы для получения отзывов. Потом для получения текста отзывов ищутся элементы {}<<div>>{} с классом {}<<\texttt{responses\_\_item\_\_message}>>{} и из него берется текст. Затем собранные отзывы отправляются в модуль работой с базой данных.
\item \textbf{Микрофинансовые организации}. Для получения отзывов отправляются запросы на \url{https://www.banki.ru/microloans/responses/ajax/responses}, где в параметры передаются код компании и номер страницы. Затем из полученного json собираются отзывы и отправляются в основную базу данных.
\end{enumerate}

\subsection{Разработка модуля сбора данных с sravni.ru}
\label{sec:orgd66ffae}
При сборе данных со sravni.ru будут отправляться запросы на их внутреннее API, которое имеет схожую структуру для всех сфер компаний. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний. Он будет получать путем отправки запроса на \url{https://www.sravni.ru/proxy-organizations/organizations} с различным значением параметра \texttt{organizationType} ({}<<bank>>{} для банков, {}<<insuranceCompany>>{} для страховых компаний и {}<<microcredits>>{} для микрофинансовых организаций). Потом полученный список компаний проверяется со списком, который сохранен в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика.

Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Потом для каждой компании получается список отзывов. Он получается путем отправки запроса по адресу \url{https://www.sravni.ru/proxy-reviews/reviews} с параметром {}<<reviewObjectType>>{} с такими же значениями, как для получения списка компаний, и идентификатором компании на сайте sravni.ru. В результате запроса получается json, в котором находится 1000 отзывов на компанию. Из этих отзывов выбираются новые отзывы с момента предыдущего сбора данных. Потом собранные данные отправляются в основную базу данных.
\subsection{Разработка модуля сбора данных с vk.com}
\label{sec:org024692e}
Для взаимодействия с API ВКонтакте был реализован класс, который делает запросы к API и подставляет обязательные параметры, такие как токен и версия API, так и параметры которые нужны для различных методов. Также этот класс регулирует количество запросов к API, так как разрешено делать не более трех запросов в секунду.

При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список отобранных заранее компаний. Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Затем для каждой компании берет публикации в группе. Для публикаций у которых разница во времени с момента предыдущего сбора данных не более недели собираются новые комментарии. Из собранных комментариев удаляются эмоджи и идентификаторы пользователей из ссылок на профиля ВКонтакте, которые имеют вид \texttt{(ID пользователя|Имя пользователя)}. Потом собранные комментарии отправляются в модуль работы с базой данных.
\section{Реализация модуля обработки текста}
\label{sec:org9ddba78}
За основу была взята модель от DeepPavlov \texttt{rubert-base-cased}~\autocite{kuratov_adaptation_2019}. К этой модели был добавлен слой для классификации предложений на 3 категории. В качестве обучающей выборки использовался использоваться набор из 20,000 предложений, размеченный экспертами на соответствие этическим практикам. Для обучения модели замораживались все ее слои, кроме двух последних.

Для удобной работы с датасетом был сделан класс \texttt{pytorch.Dataset}, который на каждой итерации модели берет новые предложения и обрабатывает (токенизирует) их для подачи модели. Классы из собранных данных ({}<<?>>{}, {}<<+>>{}, {}<<->>{}) конвертировались в 0, 1, 2. Полученные данные разделялись на обучающую и тестовую выборку в отношении 8 к 2.

Для обучения модели использовался оптимизатор \texttt{Adam}~\autocite{kingma_adam_2017} с параметром обучения 0.001. Потом для каждой эпохи обучения выполняются следующие действия:
\begin{enumerate}
\item Для обучения модели:
\begin{enumerate}
\item Устанавливается модель в режим обучения.
\item Вычисляется функция потерь на каждом батче из обучающей выборки, и производится обновление весов модели с помощью выбранного оптимизатора.
\end{enumerate}

\item Для тестирования модели:
\begin{enumerate}
\item Модель переводится в режим оценки.
\item Для каждого батча из тестовой выборки вычисляются предсказания модели и сохраняются.
\item Вычисляется f1 для тестовой выборки.
\end{enumerate}
\end{enumerate}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/test_f1.png}
\caption{\label{fig:test_f1}Результаты F1 на тестовой выборке}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/model_loss_epoch.png}
\caption{\label{fig:model_loss_epoch}Средняя ошибка модели на каждой эпохе}
\end{figure}

Потом обученная модель сохраняется и периодически запускается и обрабатывает собранные данные. Она получает их делая запросы к API, а затем отправляет результаты обработки обратно.
\section{Развертывание системы}
\label{sec:orga87627b}
Для развертывания системы каждый компонент был выделен в отдельный контейнер docker~\autocite{merkel_docker_2014}, а для оркестрации приложений существует инструмент Docker Compose, который позволяет запускать многоконтейнерные приложения с помощью YAML-файлов конфигурации. Для установки переменных окружения в контейнеры используется файл {}<<.env>>{}, в котором содержались переменные окружения для всех приложений и путь к этому файлу прописывался в параметрах \texttt{env\_file}. Для установки в названия базы данных дополнительно в конфигурации контейнеров указывалась переменная окружения \texttt{POSTGRES\_DB}.

Для реализации модуля взаимодействия с базой данной в качестве базового образа использовался \texttt{python:3.10}. Потом устанавливалась библиотека nltk и данные для работы этой библиотеки с русским языком. Затем устанавливались зависимости приложения и оно запускалось с помощью библиотеки uvicorn. Для модулей агрегации базы и сбора данных в качестве базового образа использовался \texttt{python:3.10-slim}, так как он использует меньше памяти, чем обычный.

В данной работе определен сервис {}<<database>>{}, который запускает контейнер с PostgreSQL версии 14.4. Контейнеру также присваиваются тома для хранения данных, которые будут использоваться внутри контейнера. Для обеспечения доступности сервиса в контейнере определены порты, через которые можно подключаться к базе данных. Кроме того, в конфигурации определен healthcheck, который проверяет работоспособность сервиса, выполняя команду \texttt{pg\_isready} с заданными параметрами. Этот healthcheck запускается каждые 10 секунд и проходит 5 попыток, если проверка не прошла в заданный таймаут в 5 секунд. Такой подход обеспечивает более стабильную работу контейнера и позволяет оперативно реагировать на возможные проблемы.

Для модуля работы с базой данных определен сервис {}<<api>>{}, который зависит от сервиса базы данных и начнет работу только после того, как у базы данных пройдет healthcheck.

Для модуля сбора для каждого сайта создавался новый контейнер и они запускались с помощью команды \texttt{python main.py -{}-site name}, где вместо name было название сайта и из какой сферы собирались тексты. Все сборщики данных были выделены в отдельный профиль для удобства запуска.
\section{Выводы по главе}
\label{sec:orgb74d603}
В данной главе представлена реализация системы и ее отдельных компонентов, включая базы данных и API микросервисов, согласно выявленным требованиям из первой главы. Каждый микросервис был разработан с учетом принципов микросервисной архитектуры и обеспечивает определенную функциональность, необходимую для реализации системы в целом, согласно проектированию описанном в предыдущей главе.

В результате было собрано (рис.~\ref{fig:collected_data}) 10 миллионов предложений для разных компаний и они были обработаны с помощью разработанной модели (рис.~\ref{fig:ethics_analisys}).

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/collected_data.png}
\caption{\label{fig:collected_data}График собранных предложений}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/ethics_analisys.png}
\caption{\label{fig:ethics_analisys}График оценки этичности компаний}
\end{figure}
\chapter{Тестирование системы}
\label{sec:org5110f4e}
В данной главе описывается разные методы тестирования, которые были использованы при реализации системы и форматтеры кода и инструменты статического анализа также является важным, поскольку это позволяет снизить количество ошибок и повысить качество кода.
\section{Форматирование кода}
\label{sec:org948a268}
Во время реализации системы использовались различные инструменты для форматирования кода и обеспечения его качества, такие как black, isort, pyupgrade, flake8 и mypy.

Black -- это инструмент для автоматического форматирования кода на Python. Он помогает унифицировать стиль кодирования и повысить читаемость кода. Он применяет определенный набор правил форматирования и применяет их к коду.

Isort -- это инструмент для автоматической сортировки импортов в Python-коде. Он обеспечивает единообразие сортировки импортов, что может повысить читаемость кода и уменьшить количество ошибок.

Pyupgrade -- это инструмент, который автоматически обновляет код Python до более новых версий языка, что позволяет использовать новые возможности языка и уменьшить количество устаревших функций и библиотек.

Flake8 -- это инструмент для обнаружения синтаксических и стилистических ошибок в коде Python. Он проверяет код на соответствие стандартам кодирования и выдает предупреждения, если обнаруживает ошибки.

Mypy -- это инструмент для статического анализа кода Python, который позволяет обнаруживать ошибки на этапе написания кода. Он проверяет типы переменных и аргументов функций и выдает ошибки, если они не соответствуют ожидаемым типам.

Также для автоматического форматирования кода использовался инструмент pre-commit, который позволяет форматировать кодовую базу перед отправкой в репозиторий с кодом.
\section{Тестирование системы}
\label{sec:org3831c76}
Для тестирования системы были написаны скрипты Github actions, которые запускали тестирование системы при отправки кода в репозиторий. Для тестирования использовалась библиотека pytest \autocite{krekel_pytest_2004}, также для анализа покрытия кода использовалась библиотека \texttt{pytest-cov}.
\subsection{Тестирование модуля работы с базой данных}
\label{sec:org33e9ed2}
При запуске каждого теста создавалась новая база данных и к ней применялись миграции. В зависимости от нужного теста в базу данных помещались нужные данные. Каждый метод API тестировался на верные данные, не верные данные и пустые данные, затем проверялось, что записалось в базу данных. В зависимости от требований каждого теста в базу данных помещались соответствующие данные. Затем создавался клиент для тестирования API с помощью библиотеки httpx.

Для каждого метода API тестировалось его поведение на правильные, неправильные и пустые данные. Затем проверялось, что записалось в базу данных и соответствует ожиданиям.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{img/api_coverage.png}
\caption{\label{fig:api_coverage}Покрытие кода тестами}
\end{figure}
\subsection{Тестирование модуля сбора данных}
\label{sec:org10bb3fa}
Тестирование сборщиков данных заключается в проверке корректности извлечения и обработки данных, получаемых из HTML-страниц и json. Также важно проверять сбор данных на актуальных данных сайтов. Для имитации запросов к API и сайтам использовалась библиотека requests-mock, для сохранения данных страниц использовалась библиотека \texttt{vcrpy}. Данная библиотека при первом запросе сайта сохраняет данные запроса (заголовки, параметры и так далее), а тело запроса кодирует в байты, что позволяет сократить объем. При последующих запросов вместо реального ответа будут подставляться данные предыдущего ответа. Такой способ работы с запросами позволяет ускорить процесс тестирования. Также при запуске каждого теста создавалась новая база данных.

При написании тестирования были реализованы mock-данные для API. Для подмены запросов на тестовые сначала делались запросы (или брались данные предыдущих запросов), затем с помощью библиотеки \texttt{requests-mock} данные запроса подменялись на тестовые. Потом запускались тесты, которые проверяли корректности работы функций.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{img/parser_coverage.png}
\caption{\label{fig:parser_coverage}Схема базы данных}
\end{figure}
\section{Выводы по главе}
\label{sec:orgd712ee8}
В данной главе были описаны различные виды тестирования, которые использовались при тестировании модулей для работы базой данных и сбора данных.

Были описаны тесты для сборщиков данных, которые позволяют убедиться в корректности работы при различных сценариях. Также были рассмотрены тесты для HTTP API, которые проверяют работу API на правильную обработку запросов и на корректное взаимодействие с базой данных.

Важно отметить, что тестирование должно быть проведено на всех этапах разработки, начиная с написания кода и заканчивая релизом готового продукта. Это позволит выявить ошибки на ранних стадиях разработки и снизить риски возникновения критических проблем в работе системы в будущем.

Наконец, использование форматтеров кода и инструментов статического анализа позволяет снизить количество ошибок и повысить качество кода, что также важно для обеспечения корректной работы системы.
\chapter*{Заключение}
\label{sec:org1f54a9e}
В данной работе была разработана система, которая анализирует этичность компаний на основании отзывов. Для этого были разработаны модули, которые позволяют работать с базой данных, собирать данные, агрегировать данные и обрабатывать их.

Для этого был проведен анализ литературы, разработаны модули и реализована модель.

Полученные результаты можно применять для оценки этичности компаний.

\putbibliography
\appendix
\include{tz}
\chapter{Схема базы данных}
\label{sec:org2ae7160}
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/d2/database.png}
\caption{\label{fig:database}Схема базы данных}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/d2/views_database.png}
\caption{\label{fig:database_views}Схема базы данных для агрегаций}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/d2/banki_ru.png}
\caption{\label{fig:database_banki_ru}Схема базы данных сайта banki.ru}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/d2/sravni_ru.png}
\caption{\label{fig:database_sravni_ru}Схема базы данных сайта sravni.ru}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{img/d2/vk_com.png}
\caption{\label{fig:database_vk_com}Схема базы данных сайта vk.com}
\end{figure}

\begin{landscape}
\chapter{Диаграмма классов}
\label{sec:org4d295e9}
\begin{figure}[h!]
\centering
\includegraphics[width=.9\linewidth]{img/d2/parser_class.png}
\caption{\label{fig:parser_class_diagram}Схема классов сборщиков данных}
\end{figure}
\end{landscape}
\chapter{Листинг программы}
\label{sec:orgb089896}
В данном документе представлено описание структуры репозитория, в котором находится исходный код системы, описанной и разработанной в работе.

Репозиторий программы находится по ссылке: \url{https://github.com/Samoed/EthicsAnalysis}.
\end{document}
