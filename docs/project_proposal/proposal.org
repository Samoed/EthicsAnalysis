#+STARTUP: latexpreview
#+TITLE: Site Development for automatic collection analysis and visualization of company ethical behavior
#+AUTHOR: Roman Solomatin
#+LANGUAGE: EN
#+LATEX_CLASS: ProjectProposal
#+LATEX_CLASS_OPTIONS: [PI]
#+bibliography: ../library.bib
#+cite_export: biblatex
#+OPTIONS: toc:nil H:4 ':t

In this paper, an analysis of the ethics of different companies is carried out.

* Introduction
The trustworthiness of companies has long been a matter of concern, particularly with respect to their behavior in contentious situations and their delivery of customer-centric services. In recent years, there has been a growing emphasis on assessing the ethicality of companies[cite:@mure_esg_2021; @miralles-quiros_esg_2019; @climent_ethical_2018], particularly within the banking sector and through the lens of Environmental, Social and Governance (ESG) factors. The need for such assessments has become increasingly urgent as society continues to grapple with the consequences of corporate misconduct and the broader impact of corporate activities on society.

Currently, there are several services that purport to assess a company's ethics, but these assessments are often based on court cases and other official records rather than on customer feedback. This has led to a situation where individuals must conduct their own research to determine the ethics of a particular company. This research often involves reviewing customer feedback from various websites, which can be time-consuming and may not always provide a comprehensive or accurate picture of a company's ethical practices.

To address this issue, there have been recent calls for the development of a system that would collect and analyze customer feedback from multiple websites to provide a more comprehensive and accurate assessment of a company's ethical practices. Such a system will be designed to automatically collect and analyze customer feedback from a variety of sources, including social media and review sites. The collected data could then be analyzed using various techniques, such as natural language processing, to identify patterns and trends related to a company's ethical practices. The resulting analysis could then be used to develop a more robust and reliable system for assessing the ethicality of companies.
* Literature Review
** BERT
BERT\nbsp{}[cite:@devlin2018bert] (Bidirectional Encoder Representations from Transformers) is a language model that has been shown to be highly effective for natural language processing tasks such as text classification, question answering, and named entity recognition. BERT is based on the transformer architecture, introduced in the paper "Attention is All You Need" [cite:@NIPS2017_3f5ee243], that uses self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional recurrent neural network architectures.

The BERT model is trained using a technique called "masked language modeling," in which the model is trained to predict the value of certain randomly masked tokens in a sentence, given the surrounding context. This training process allows the model to learn the relationships between words in a sentence and to represent each word in a high-dimensional vector space called an embedding. These embeddings capture the meaning of the words in a sentence and can be used to represent the sentence as a whole.

Unlike simple machine learning algorithms such as, TF-IDF [cite:@jones1972statistical], word bag[cite:@doi:10.1080/00437956.1954.11659520] and word2vec [cite:@mikolov2013efficient] sequences of words and sentences are encoded by vector (embedding) of fixed length. This allows you to efficiently analyze text with context understanding, which will help in review analysis.
One of the main reasons for BERT's superior performance is its ability to handle bidirectional context. Unlike ELMO [cite:@elmo] and GPT[cite:@radford2019language], which are unidirectional models, BERT is trained to consider context on both the left and right sides of a word, resulting in a more accurate representation of word meaning in a sentence.

BERT can be used to obtain embeddings to assess the tone of reviews about companies. These embeddings can then be used to train a classifier to predict the tone of the reviews. The classifier can be trained to predict whether a review is positive, negative, or neutral. Once trained, the classifier can be used to predict the tone of new reviews, providing a reliable and efficient way to assess the tone of reviews about companies.
** Sentence BERT
Sentence BERT\nbsp{}[cite:@reimers-2019-sentence-bert] is a modified version of the BERT model that exploits the capabilities of the BERT model by inputting pairs of sentences and averaging their output representations. The primary benefit of this modification is the ability to compare sentence embeddings in an independent and computationally efficient manner. The primary advantage of Sentence BERT over traditional BERT models is its ability to compare sentence embeddings independently and without the need for recalculating them each time. For example, when searching for similar sentences among a set of 10,000, traditional BERT models would require 50 million calculations of different sentence pairs, which would take approximately 50 hours. In contrast, Sentence BERT calculates the embedding of each sentence separately and then compares them, which takes only about 5 seconds. This makes Sentence BERT a computationally efficient solution for comparing sentence embeddings and identifying similarities.
** CLIP
CLIP[cite:@radford2021learning] (Contrastive Language-Image Pre-training) is a pre-training technique that uses a transformer-based models to learn general-purpose embeddings. The CLIP model is trained on a large dataset of image-text pairs and is capable of learning a wide range of visual and linguistic concepts by predicting the text that corresponds to a given image input.

The model uses Visual Transformer (ViZ) [cite:@dosovitskiy2020image] to learn representations of images. The ViZ is trained to understand and generate images, while the transformer is trained to understand and generate text. The combination of these two architectures allows the CLIP model to learn visual and linguistic concepts simultaneously.

One of the key advantages of CLIP is its ability to learn embeddings that are not specific to a particular task or domain. In addition, CLIP can be fine-tuned on a task-specific dataset to improve performance on specific tasks. This method allows you to connect the spaces of two different sources of information. For example, this model can be adapted to connect sentences from different fields.
* Methods
* Results Anticipated
* Conclusion
#+latex: %\nocite{*}
#+LATEX: \putbibliography
#+LATEX: \appendix
